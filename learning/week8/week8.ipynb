{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d356c855",
   "metadata": {},
   "source": [
    "# Week 8: Advanced Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984bbb5c",
   "metadata": {},
   "source": [
    "Complex regression types like Polynomial, Ridge, Lasso, Elastic Net, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44adee81",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/machine-learning/ml-different-regression-types/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe70192e",
   "metadata": {},
   "source": [
    "Regression Analysis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Regularization Techniques\n",
    "\n",
    "Types of Regression in ML\n",
    "\n",
    "Linear\n",
    "\n",
    "Logistic\n",
    "\n",
    "Polynomial\n",
    "\n",
    "Softmax Regression\n",
    "\n",
    "Ridge Regression\n",
    "\n",
    "Lasso Regression \n",
    "\n",
    "Elastic Net Regression\n",
    "\n",
    "Need for Regression\n",
    "\n",
    "Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7de378",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/machine-learning/ml-linear-regression/\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2022/01/different-types-of-regression-models/\n",
    "\n",
    "https://scikit-learn.org/stable/modules/linear_model.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60eab64",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664adb8a",
   "metadata": {},
   "source": [
    "#### Polynomial Regression\n",
    "\n",
    "Concept: Understanding how to model non-linear relationships by introducing polynomial features $$x^2, x^3$$\n",
    "\n",
    "Key Challenge: The risk of Overfitting with high-degree polynomials.\n",
    "\n",
    "What if our data is actually more complex than a simple straight line? Surprisingly, we can actually use a linear model to fit nonlinear data. A simple way to do this is to add powers of each feature as new features, then train a linear model on this extended set of features. This technique is called Polynomial Regression.The equation below represents a polynomial equation:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\cdots + \\beta_n x^n + \\varepsilon.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1252e32",
   "metadata": {},
   "source": [
    "### Regularized Regression \n",
    "Concept: Introduction to the concept of a penalty term or regularizer added to the loss function to constrain model complexity.\n",
    "\n",
    "##### Ridge Regression \n",
    "$$L_2 Regularization$$\n",
    "\n",
    " Rigid Regression is a regularized version of Linear Regression where a regularized term is added to the cost function. This forces the learning algorithm to not only fit the data but also keep the model weights as small as possible. Note that the regularized term should not be added to the cost function during training. Once the model is trained, you want to evaluate the model's performance using the unregularized performance measure. The formula for ridge regression is:\n",
    "$$J(\\mathbf{\\theta}) = \\text{MSE}(\\mathbf{\\theta}) + \\alpha \\frac{1}{2}\\sum_{i=1}^{n} \\theta_i^2$$\n",
    "\n",
    "Explanation of the Formula Components:\n",
    "\n",
    "* $J(\\mathbf{\\theta})$: The **Cost Function** (or loss function) that the model minimizes.\n",
    "* $\\text{MSE}(\\mathbf{\\theta})$: The **Mean Squared Error** component, which measures the model's performance on the training data. This is the standard part of the cost function for linear regression.\n",
    "* $\\alpha \\frac{1}{2}\\sum_{i=1}^{n} \\theta_i^2$: The **Regularization Term** (or penalty term).\n",
    "    * $\\mathbf{\\theta}$: The vector of model **parameters** (the weights or coefficients).\n",
    "    * $\\theta_i$: The $i$-th **parameter** (excluding the intercept, $\\theta_0$).\n",
    "    * $\\sum_{i=1}^{n} \\theta_i^2$: The sum of the squared parameters (**L2-norm**).\n",
    "    * $\\alpha$ (alpha): The **regularization hyperparameter** that controls the strength of the penalty. A larger $\\alpha$ forces the model to use smaller weights.\n",
    "    * $\\frac{1}{2}$: A constant factor often included for mathematical convenience, ensuring that the derivative of the term is simply $\\alpha\\sum \\theta_i$.\n",
    "\n",
    "Ridge Regression, also known as **L2 Regularization**, adds this penalty term to prevent **overfitting** by keeping the model weights small.\n",
    "\n",
    "- Adds the squared magnitude of coefficients to the loss function.\n",
    "- Shrinks coefficients towards zero, reducing variance.\n",
    "- Supplementary: The λ (or α) hyperparameter and its role."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49f403a",
   "metadata": {},
   "source": [
    "#### Lasso Regression \n",
    "$$L_1 ​Regularization$$\n",
    "\n",
    " Similar to Ridge Regression, Lasso (Least Absolute Shrinkage and Selection Operator) is another regularized version of Linear regression : it adds a regularized term to the cost function, but it uses the l1 norm of the weighted vector instead of half the square of the l2 term Lasso regression is given as:\n",
    "\n",
    "$$J(\\mathbf{\\theta}) = \\text{MSE}(\\mathbf{\\theta}) + \\alpha \\sum_{i=1}^{n} |\\theta_i|$$\n",
    "\n",
    "Explanation of the Formula Components\n",
    "\n",
    "This equation is very similar to the Ridge Regression cost function, but it uses the **absolute value of the weights** instead of the square, which has a different effect on the model.\n",
    "\n",
    "* $J(\\mathbf{\\theta})$: The **Cost Function** that the model minimizes.\n",
    "* $\\text{MSE}(\\mathbf{\\theta})$: The **Mean Squared Error** component (the loss from prediction accuracy).\n",
    "* $\\alpha \\sum_{i=1}^{n} |\\theta_i|$: The **Regularization Term** (**L1 Penalty**).\n",
    "    * $\\mathbf{\\theta}$: The vector of model **parameters** (weights).\n",
    "    * $\\sum_{i=1}^{n} |\\theta_i|$: The sum of the absolute values of the parameters (**L1-norm**).\n",
    "    * $\\alpha$ (alpha): The **regularization hyperparameter**.\n",
    "\n",
    "The key distinction of Lasso ($\\sum |\\theta_i|$) is its ability to drive the coefficients of unimportant features to **exactly zero**, effectively performing **feature selection**.\n",
    "\n",
    "- Adds the absolute magnitude of coefficients to the loss function.\n",
    "- Can drive some coefficients exactly to zero, effectively performing Feature Selection.\n",
    "- Supplementary: The λ (or α) hyperparameter and its role.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf80aa76",
   "metadata": {},
   "source": [
    "## Regularization Comparison: L2 vs. L1\n",
    "\n",
    "| Feature | L2 (Ridge Regression) | L1 (Lasso Regression) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Penalty Term** | Sum of **squared** coefficients ($\\sum \\theta_i^2$). | Sum of the **absolute values** of coefficients ($\\sum |\\theta_i|$). |\n",
    "| **Effect on Coefficients** | **Drives coefficients down** (shrinks them) toward zero. | **Drives coefficients of unimportant features to *exactly* zero.** |\n",
    "| **Primary Benefit** | Reduces variance and prevents **overfitting** by keeping all model weights small. | Performs **automatic feature selection** by eliminating unimportant features. |\n",
    "| **Outcome** | All original features remain in the model, but with reduced influence. | The resulting model is simpler and more interpretable as it only includes the most relevant features. |\n",
    "\n",
    "***\n",
    "### Key Takeaway on Magnitude and Prediction\n",
    "\n",
    "> \"so the more magnitude a feature has, the more it will be felt and the less or close to 0, it will not affect the prediction\"\n",
    "\n",
    "* **L2 (Ridge):** Penalizes *large* coefficients heavily because the penalty is proportional to $\\theta_i^2$. It forces large coefficients to shrink, meaning that a feature with a very high magnitude (influence) will have its contribution to the prediction ***reduced***.\n",
    "* **L1 (Lasso):** Also penalizes large coefficients, but its unique geometry allows it to push coefficients for irrelevant features **all the way to zero** instead of just close to zero. This means that features driven to $0$ truly have **zero effect** on the prediction, fulfilling the goal of **feature selection**.\n",
    "\n",
    "Basically:\n",
    "\n",
    "- L2 (Ridge Regression): Correctly drives coefficients down or reduces their magnitude using a penalty term to penalize large weights. This keeps the overall model weights small to prevent overfitting.\n",
    "\n",
    "- L1 (Lasso Regression): Correctly drives coefficients of unimportant features to 0 (exactly zero) using its penalty term, which allows it to be used for feature selection.\n",
    "\n",
    "the more magnitude a coefficient has, the more the feature will influence the prediction. L1 and L2 reduce this influence, but L1 goes the extra step of setting it to zero for useless features.\n",
    "\n",
    "- Coefficient≈0⟹No/Minimal effect on prediction.\n",
    "\n",
    "- Coefficient=0⟹Zero effect on prediction (Lasso’s unique ability)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b878d5",
   "metadata": {},
   "source": [
    "#### Role of hyperparameter lambda and alpha \n",
    "\n",
    "The hyperparameter λ (lambda), often represented as α (alpha) in practical implementations like Python's scikit-learn, is the single most important control mechanism in both L1 (Lasso) and L2 (Ridge) regularization. It controls the strength of the penalty applied to the model's coefficients.\n",
    "\n",
    "The hyperparameter λ (or α) controls the strength of the penalty term added to the loss function. This penalty manages the bias-variance trade-off to ensure the model generalizes well, mitigating the risks of both overfitting (too complex) and underfitting (too simple).\n",
    "\n",
    "##### Role of λ / α in Regularization\n",
    "$$Cost=Loss(Data Fit)+λ×Penalty(Model Complexity)$$\n",
    "\n",
    "The λ or α value is the constant that multiplies the penalty term, determining the trade-off between:\n",
    "1. Fitting the data well (minimizing the original Loss term, which risks overfitting).\n",
    "2. Keeping the model simple (minimizing the Penalty term, which risks underfitting).\n",
    "\n",
    "### Effect of $\\lambda$ / $\\alpha$ on Model Behavior\n",
    "\n",
    "| $\\lambda$ / $\\alpha$ Value | Regularization Strength | Effect on Coefficients | Model Outcome |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **$\\lambda = 0$** | **Zero** (No regularization) | Coefficients are unconstrained. | Model reverts to **Ordinary Least Squares (OLS)**. High risk of overfitting. |\n",
    "| **Small $\\lambda$** | **Weak** | Small penalty; coefficients are shrunk slightly. | Model is complex. Still risks overfitting. |\n",
    "| **Optimal $\\lambda$** | **Balanced** | Achieves the best trade-off between bias and variance. | **Optimal Generalization** (the goal). |\n",
    "| **Large $\\lambda$** | **Strong** | Coefficients are heavily penalized (pushed very close to zero). | Model becomes overly simple. High risk of **underfitting** (high bias). |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bce060",
   "metadata": {},
   "source": [
    "### $\\lambda$ / $\\alpha$ Specific Role for L1 and L2\n",
    "\n",
    "| Regularization Type | Penalty Term | The Role of $\\lambda$ |\n",
    "| :--- | :--- | :--- |\n",
    "| **L2 (Ridge)** | $\\lambda \\sum \\theta_i^2$ (Sum of **squared** coefficients) | $\\lambda$ controls the overall **magnitude** of coefficients, making them small without eliminating any features. |\n",
    "| **L1 (Lasso)** | $\\lambda \\sum \\theta_i$ (Sum of **absolute values** of coefficients) | $\\lambda$ controls the **number of features used**. A higher $\\lambda$ forces more unimportant features' coefficients to become **exactly zero** (Feature Selection). |\n",
    "\n",
    "Specific Action on Model Coefficients:\n",
    "1. L2 Regularization (Ridge)\n",
    " - Primary Action: The λ value controls the overall magnitude of the coefficients.\n",
    " - Mechanism: It works by making coefficients small or shrinking them toward zero. Coefficients with very high weights are penalized more aggressively to reduce their individual influence on the prediction.\n",
    " - Result: Coefficients are reduced but never set exactly to zero, meaning no features are eliminated. The model maintains all features but with smaller, more evenly distributed weights.\n",
    "\n",
    "2. L1 Regularization (Lasso)\n",
    " - Primary Action: The λ value controls feature selection and model sparsity.\n",
    " - Mechanism: It penalizes the absolute value of the coefficients. While it shrinks all coefficients, it is mathematically more aggressive, forcing the weights of irrelevant or redundant features to become exactly zero.\n",
    " - Result: It effectively removes the effect of useless features from the prediction, achieving the goal of feature selection.\n",
    "\n",
    " | Regularization Type | Coefficient Treatment | Feature Status |\n",
    "| :--- | :--- | :--- |\n",
    "| **L2 (Ridge)** | Shrinks coefficients (minimizes their effects) | Features are retained but never zero|\n",
    "| **L1 (Lasso)** | Shrinks coefficients and sets irrelevant ones to zero |Features are eliminated / feature selection |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc99234",
   "metadata": {},
   "source": [
    "### Elastic Net (Optional but Recommended): A hybrid of Ridge and Lasso.\n",
    "\n",
    "ElasticNet is middle ground between Lasso and Ridge Regression techniques.The regularization term is a simple mix of both Rigid and Lasso's regularization term. when r=0, Elastic Net is equivalent to Rigid Regression and when r=1, Elastic Net is equivalent to Lasso Regression.The expression for Elastic Net Regression is given as:\n",
    "$$J(\\boldsymbol{\\theta}) = \\text{MSE}(\\boldsymbol{\\theta}) + r\\alpha\\sum_{i=1}^{n} |\\theta_i| + \\frac{1-r}{2}\\alpha\\sum_{i=1}^{n} \\theta_i^2$$\n",
    "\n",
    "This equation represents the cost function for Elastic Net Regularization, which combines the penalty terms from Lasso (L1) and Ridge (L2) regression.\n",
    "\n",
    "The formula you provided is the **cost function** for the **Elastic Net Regularization** model. It consists of three main parts: the loss function ($\\text{MSE}$), the $L_1$ penalty (Lasso), and the $L_2$ penalty (Ridge).\n",
    "\n",
    "The model's goal is to find the set of parameter values, $\\boldsymbol{\\theta}$, that **minimizes** this function.\n",
    "\n",
    "#### 1. Loss Term: $\\text{MSE}(\\boldsymbol{\\theta})$\n",
    "\n",
    "| Term | Name | Purpose |\n",
    "| :--- | :--- | :--- |\n",
    "| $$\\text{MSE}(\\boldsymbol{\\theta})$$ | **Mean Squared Error** (MSE) | This is the primary **loss function**. It measures the average of the squared differences between the predicted values ($$\\hat{y}$$) and the actual target values ($$y$$). Its purpose is to make the model's predictions as close as possible to the observed data. |\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Regularization Terms (The Penalty)\n",
    "\n",
    "These two terms are added to **prevent overfitting** by penalizing large coefficients, which helps to simplify the model.\n",
    "\n",
    "##### A. $L_1$ Penalty (Lasso)\n",
    "\n",
    "| Term | Name | Effect |\n",
    "| :--- | :--- | :--- |\n",
    "| $$r\\alpha\\sum_{i=1}^{n}\\theta_i$$ | **$$L_1$$ Regularization** | The sum of the absolute values of the coefficients. It forces some coefficients ($$\\theta_i$$) to be **exactly zero**, which is highly effective for **automatic feature selection** by dropping irrelevant features. |\n",
    "\n",
    "##### B. $L_2$ Penalty (Ridge)\n",
    "\n",
    "| Term | Name | Effect |\n",
    "| :--- | :--- | :--- |\n",
    "| $$\\frac{1-r}{2}\\alpha\\sum_{i=1}^{n} \\theta_i^2$$ | **$L_2$ Regularization** | The sum of the squared values of the coefficients. It **shrinks the magnitude** of the coefficients toward zero (but rarely to exactly zero), which stabilizes the model and reduces its sensitivity to noise. |\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Hyperparameters (The Control Knobs)\n",
    "\n",
    "These values are set **before** the training process to control the learning.\n",
    "\n",
    "| Term | Name | Role |\n",
    "| :--- | :--- | :--- |\n",
    "| $$\\alpha$$ | **Regularization Strength** | The **overall penalty strength**. A larger $\\alpha$ means a stronger penalty, leading to smaller coefficients and a simpler model. If $$\\alpha=0$$, there is no regularization. |\n",
    "| $$r$$ | **$L_1$ Ratio** | The **mixing parameter** that balances the $L_1$ and $L_2$ penalties ($0 \\le r \\le 1$). |\n",
    "\n",
    "#### The Role of $r$:\n",
    "\n",
    "* If **$r = 1$**: The model is pure **Lasso Regression** (only $L_1$ penalty is active).\n",
    "* If **$r = 0$**: The model is pure **Ridge Regression** (only $L_2$ penalty is active).\n",
    "* If **$0 < r < 1$**: The model is **Elastic Net**, combining the benefits of both Lasso and Ridge.\n",
    "\n",
    "Elastic Net is often preferred over Lasso when there are many highly correlated features, as Lasso tends to randomly select only one of them, while Elastic Net will keep them all but shrink their weights, providing a more stable and robust model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2b698c",
   "metadata": {},
   "source": [
    "### Effect of hyperparameter\n",
    "\n",
    "The two hyperparameters, **$\\alpha$** and **$r$**, are the \"control knobs\" for the Elastic Net model. They directly determine the **overall strength** and the **type** of regularization applied.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Effect of $\\alpha$ (Overall Regularization Strength)\n",
    "\n",
    "$\\boldsymbol{\\alpha}$ controls the total magnitude of the penalty applied to the coefficients. It manages the **bias-variance tradeoff**.\n",
    "\n",
    "| $\\alpha$ Value | Impact on Penalty | Effect on Model | Risk |\n",
    "| :---: | :--- | :--- | :--- |\n",
    "| $\\alpha \\rightarrow 0$ | Penalty is negligible. | Behaves like standard **Linear Regression**. Model complexity is high. | High **Overfitting** |\n",
    "| $\\alpha \\rightarrow \\infty$ | Penalty dominates the cost function. | Coefficients are aggressively shrunk towards zero. Model complexity is low. | High **Underfitting** |\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Effect of $r$ (Mixing Parameter or $L_1$ Ratio)\n",
    "\n",
    "$r$ controls the blend between the $L_1$ (Lasso) and $L_2$ (Ridge) penalties, where $0 \\le r \\le 1$.\n",
    "\n",
    "| $r$ Value | Model Type | Dominant Penalty Term | Primary Effect |\n",
    "| :---: | :--- | :--- | :--- |\n",
    "| **$r = 1$** | **Lasso Regression** | $$r\\alpha\\sum_{i=1}^{n} \\theta_i$$ | Strong **Feature Selection** (forces coefficients to $\\mathbf{0}$). |\n",
    "| **$r = 0$** | **Ridge Regression** | $\\frac{1-r}{2}\\alpha\\sum_{i=1}^{n} \\theta_i^2$ | **Coefficient Shrinkage** (reduces magnitude for stability). |\n",
    "| **$0 < r < 1$** | **Elastic Net** | Both terms are active. | **Blend** of both effects, offering both stability and feature selection. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8288522d",
   "metadata": {},
   "source": [
    "#### Differences and Similarities of Ridge, Lasso, and Elastic Net\n",
    "- The strength of the penalty is controlled by α in all three.\n",
    "- The mix of the penalties in Elastic Net is controlled by r. By tuning r, you ensure the model is stable (like Ridge) without sacrificing the ability to remove irrelevant features (like Lasso).\n",
    "\n",
    "Here is a comparison of the three main regularization techniques:\n",
    "\n",
    "| Model | Penalty Term ($\\mathbf{P(\\boldsymbol{\\theta})}$) | Core Mechanism | Primary Benefit |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Ridge Regression** | $L_2 = \\sum_{i=1}^{n} \\theta_i^2$ | **Shrinks** coefficients **towards zero** but rarely to exactly zero. | **Model Stability**, especially when features are highly correlated (multicollinearity). |\n",
    "| **Lasso Regression** | $L_1 = \\sum_{i=1}^{n} \\theta_i$ | **Forces** the coefficients of **irrelevant features to exactly zero**. | **Automatic Feature Selection** and model interpretability (sparsity). |\n",
    "| **Elastic Net** | $r L_1 + \\frac{1-r}{2} L_2$ | **Combines** both $L_1$ and $L_2$ penalties based on the mixing ratio $r$. | **Best of both:** Offers both **feature selection** and **enhanced stability** (outperforms Lasso when correlated features exist). |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1371d7",
   "metadata": {},
   "source": [
    "### When to use and how to use\n",
    "In practice, the best approach is often to use the Elastic Net. Because Ridge and Lasso are just special cases of Elastic Net (by setting r=0 or r=1, respectively), a robust cross-validation procedure on Elastic Net will naturally find the optimal blend, including pure Lasso or pure Ridge, if they are the best fit for your data.\n",
    "\n",
    "## Factors to Consider When Choosing a Model\n",
    "\n",
    "| Factor | Description | Best Choice | Rationale |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Feature Set Size ($\\mathbf{p}$)** | The number of features relative to the number of samples ($\\mathbf{n}$). When $\\mathbf{p \\gg n}$ (many features, few samples). | **Lasso or Elastic Net** | Feature selection is necessary to simplify the model and resolve the underdetermined problem. |\n",
    "| **Multicollinearity** | Whether two or more features are highly correlated with each other. | **Ridge or Elastic Net** | Ridge handles correlated features well by shrinking their coefficients equally, providing stability. |\n",
    "| **Sparsity/Interpretability** | Whether you need a model that only relies on a small, select group of features. | **Lasso or Elastic Net** ($\\mathbf{r}$ closer to $\\mathbf{1}$) | Forces irrelevant coefficients to exactly zero, making the model simpler and easier to interpret. |\n",
    "| **Belief in True Model** | Do you believe all features contribute, or only a few contribute significantly? | **Ridge** (many small effects) or **Lasso** (few large effects) | Ridge for stable, dispersed coefficient values; Lasso for sparse, dominant features. |\n",
    "\n",
    "---\n",
    "\n",
    "## Regularization Mechanism Summary\n",
    "\n",
    "* **Ridge Regression** ($L_2$ Penalty): Shrinks coefficients **toward zero** to stabilize the model.\n",
    "* **Lasso Regression** ($L_1$ Penalty): Forces irrelevant coefficients to **exactly zero**, effectively picking only those features that strongly impact the model.\n",
    "* **Elastic Net**: A mix of both ($L_1$ and $L_2$) that ensures the model is more **stable** (like Ridge) without losing the ability to **remove irrelevant features** (like Lasso), depending on the strength ($\\alpha$) and ratio ($r$) of the penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdd980c",
   "metadata": {},
   "source": [
    "### When to Use and How to Use Regularization Techniques\n",
    "\n",
    "#### 1. Ridge Regression ($L_2$ Penalty)\n",
    "\n",
    "#### When to Use:\n",
    "* You have **multicollinearity** (highly correlated features) and you want a **stable model**.\n",
    "* You believe **all features are relevant** but want to reduce their overall impact and stabilize their weights.\n",
    "* You are primarily focused on **prediction accuracy and stability**, not feature selection.\n",
    "\n",
    "#### How to Use:\n",
    "* Set the hyperparameter $\\boldsymbol{\\alpha}$ (or $\\boldsymbol{\\lambda}$) via **cross-validation**.\n",
    "* $\\boldsymbol{\\alpha}$ controls the strength: choose the $\\boldsymbol{\\alpha}$ that minimizes the cross-validated error.\n",
    "\n",
    "***\n",
    "\n",
    "#### 2. Lasso Regression ($L_1$ Penalty)\n",
    "\n",
    "#### When to Use:\n",
    "* You suspect many of your features are **irrelevant noise**.\n",
    "* You want to perform **automatic feature selection** and create a sparse, highly interpretable model.\n",
    "* You have a dataset where the number of features ($\\mathbf{p}$) is much larger than the number of observations ($\\mathbf{n}$) ($\\mathbf{p \\gg n}$).\n",
    "\n",
    "#### How to Use:\n",
    "* Set the hyperparameter $\\boldsymbol{\\alpha}$ via **cross-validation**.\n",
    "* $\\boldsymbol{\\alpha}$ controls the sparsity: a higher $\\boldsymbol{\\alpha}$ will set more coefficients to **zero**.\n",
    "\n",
    "***\n",
    "\n",
    "#### 3. Elastic Net Regularization ($L_1 + L_2$ Penalty)\n",
    "\n",
    "#### When to Use:\n",
    "* This is the **default recommendation** when you have no strong prior knowledge.\n",
    "* You have many **correlated features** (like Ridge), *but* you also need **feature selection** (like Lasso).\n",
    "* You encounter the issue where Lasso selects only one feature from a group of correlated features and you want to keep them all with reduced magnitude.\n",
    "\n",
    "#### How to Use:\n",
    "* Requires tuning **two** hyperparameters using **cross-validation** on a 2D grid:\n",
    "    * $\\boldsymbol{\\alpha}$: The overall regularization strength (how strong the penalty is).\n",
    "    * $\\mathbf{r}$ ($L_1$ ratio): The mix between $L_1$ and $L_2$ (how much feature selection vs. stability).\n",
    "\n",
    "***\n",
    "\n",
    "### Understanding $p \\gg n$ (Features $\\gg$ Observations)\n",
    "\n",
    "The statement \"**You have a dataset where the number of features ($\\mathbf{p}$) is much larger than the number of observations ($\\mathbf{n}$)**\" is crucial in statistical modeling. It describes a situation known as a **high-dimensional problem** or when a model is **underdetermined**.\n",
    "\n",
    "#### Explanation\n",
    "\n",
    "* $\\mathbf{p}$ (Features/Variables): The number of independent variables (columns) in your dataset.\n",
    "    * *Example:* Predicting house price ($\\mathbf{y}$) using $5,000$ characteristics ($\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_{5000}$), so $\\mathbf{p}=5000$.\n",
    "* $\\mathbf{n}$ (Observations/Samples): The number of data points (rows) you have.\n",
    "    * *Example:* Data for only $50$ houses, so $\\mathbf{n}=50$.\n",
    "* When $\\mathbf{p \\gg n}$ (e.g., $p=5000, n=50$), you have **far more potential explanatory variables than data points** to reliably estimate their effects.\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "This scenario leads to several major issues in standard linear regression (Ordinary Least Squares, or OLS):\n",
    "\n",
    "* **Non-Unique Solution:** In OLS, the exact solution for the coefficients is mathematically impossible because the system is **underdetermined**. There are **infinitely many coefficient vectors** ($\\boldsymbol{\\theta}$) that could perfectly fit the limited training data.\n",
    "* **Overfitting:** With many more features than samples, the model tends to learn the noise of the small dataset **perfectly**, leading to very high variance and extremely poor performance on unseen data.\n",
    "* **Multicollinearity:** It is highly likely that many features will be highly correlated, which **destabilizes the coefficient estimates** (they can swing wildly).\n",
    "\n",
    "#### The Solution (Regularization)\n",
    "\n",
    "Regularization methods are specifically designed to handle $\\mathbf{p \\gg n}$:\n",
    "\n",
    "* **Lasso ($L_1$)**: Is excellent here because it performs **feature selection**, immediately forcing the coefficients of many superfluous features to **exactly zero**. This reduces the effective number of features, $\\mathbf{p}$, down to a manageable size.\n",
    "* **Elastic Net**: Is often the better choice because it combines the **feature selection** of Lasso with the **stability** of Ridge, which helps manage the high likelihood of correlated features in this high-dimensional setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e640b523",
   "metadata": {},
   "source": [
    "#### When to use - revisited \n",
    "\n",
    "Ridge Regression (L2):\n",
    "\n",
    "When to Use: You are correct. Use Ridge when you have multicollinearity (correlated features) and your domain knowledge suggests all features are relevant. The goal is to shrink the magnitude of all coefficients toward zero to stabilize the weights and improve model accuracy, without explicitly removing any features (no \"cherry picking\").\n",
    "\n",
    "Lasso Regression (L1):\n",
    "\n",
    "When to Use: You are correct. Use Lasso when your core belief is that the final model should be sparse because many features are irrelevant. This is highly beneficial in high-dimensional data where the number of features (p) is much greater than the number of observations (n) (e.g., p=500 columns, n=50 rows). The key mechanism is feature selection by forcing coefficients to exactly zero.\n",
    "\n",
    "Elastic Net:\n",
    "\n",
    "When to Use: You are correct. Elastic Net is the default compromise. It ensures irrelevant features are removed (Lasso benefit) while simultaneously managing multicollinearity to improve stability (Ridge benefit). It solves the specific Lasso drawback where, if you have a group of highly correlated features, Lasso often arbitrarily picks one and drops the rest. Elastic Net will tend to keep the entire group of correlated features while shrinking their weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd728e1",
   "metadata": {},
   "source": [
    "Concept of Model Evaluation and Use of Metrics like RMSE and MAE\n",
    "\n",
    "Learn how to calculate and interpret key model evaluation metrics like Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb9b070",
   "metadata": {},
   "source": [
    "RMSE And MAE\n",
    "\n",
    "RMSE (Root Mean Square Error) and MAE (Mean Absolute Error) are both model evaluation metrics that measure the average magnitude of errors between predicted and actual values, but RMSE penalizes larger errors more heavily by squaring them, making it sensitive to outliers, while MAE treats all errors equally by taking their absolute values, resulting in a more robust measure. RMSE's units are the same as the data, while MAE is also in the original units. \n",
    "\n",
    "Mean Absolute Error or MAE \n",
    "- MAE is the average of the absolute differences between the predicted and actual values.\n",
    "- Robust to Outliers: MAE is less sensitive to extreme values (outliers) because it takes the absolute value of errors, rather than squaring them.\n",
    "- Equal Weighting: It assigns the same weight to all errors, meaning an error of 10 has the same impact as an error of 40.\n",
    "- Interpretation: Provides a straightforward average error in the original units of the data\n",
    "- Penalty term: Absolute value, treats all errors equally\n",
    "$$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_{\\text{pred}, i} - y_{\\text{true}, i}|$$\n",
    "\n",
    "\n",
    "Root Mean Square Error\n",
    "- RMSE is the square root of the average of the squared errors. \n",
    "- Sensitive to Outliers: Because it squares the errors, larger errors contribute disproportionately to the RMSE, making it a good measure when large errors are particularly undesirable as it is highly sensitive to outliers\n",
    "- Penalizes Larger Errors: Gives more weight to larger errors, meaning being off by 10 is significantly worse than being off by 5.\n",
    "- Interpretation: Also reports the error in the original units of the data. \n",
    "- Penalty term: Squared value, Penalizes large errors disproportionately (magnifies the impact of outliers).\n",
    "$$\\text{RMSE} = \\sqrt{\\frac{\\sum_{i=1}^{n} (y_{\\text{pred}, i} - y_{\\text{true}, i})^2}{n}}$$\n",
    "\n",
    "When to use: \n",
    "1. Choose MAE when:\n",
    "- You want a metric that is less affected by outliers and gives equal weight to all errors. \n",
    "- When the data does not have many outliers, or you want to give equal weight across all errors as ot is less sensitive to outliers and provides a straightforward average error.\n",
    "- Used if the data does not have many outliers or you feel equal weighting is appropriate.\n",
    "2. Choose RMSE when:\n",
    "- You want to penalize larger errors more severely, which is useful when outliers are a significant concern and large deviations are particularly problematic for your application. \n",
    "- When you expect larger errors or have a lot of outliers/larger deviations, and you want to penalize those errors more severely. It penalizes larger errors more severely by squaring them, making it a good measure when large deviations are particularly problematic.\n",
    "- Used when you expect larger errors and need to account for larger deviations.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
