{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91da06ab",
   "metadata": {},
   "source": [
    "# Week 3 Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7a565b",
   "metadata": {},
   "source": [
    "Linear Regression\n",
    "\n",
    "https://www.youtube.com/watch?v=nk2CQITm_eo\n",
    "\n",
    "Main ideas behind it\n",
    "1. Least squares to fit a line to the data\n",
    "2. Calculate r^2\n",
    "3. Calculate a p-value for r^2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd74eb7c",
   "metadata": {},
   "source": [
    "Goal is to use mouse weight to predict mouse size\n",
    "1. Draw a line in the data\n",
    "2. Measure the distance from the line to the data, square each distance and then add them up\n",
    "    - Residual = distance from a line to a data point\n",
    "3. Rotate the line a bit\n",
    "    - With the new line. measure the residuals, square them and then sum up the squares.\n",
    "4. Rotate the line a bit more\n",
    "    - Sum up the square residuals\n",
    "\n",
    "Basically rotate, and then sum up the squared residuals.\n",
    "The rotation with the least squares will be the one used to fit to the data.\n",
    "\n",
    "Y-axis intercept and a slope. Since the slope is no 0, it means that knowing a mouse's weight will help us make a guess about that mouse's size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d91d44c",
   "metadata": {},
   "source": [
    "Calculating R-squared\n",
    "\n",
    "1. First calculate the average mouse size (y)\n",
    "2. From the average mouse size, calculate the squared residuals.\n",
    "    - Measure the distance from the mean to the data point and square it then add those squares together.\n",
    "    - aka SS(Mean) or sum of squares around the mean\n",
    "        - SS(Mean) = (data-mean)^2\n",
    "        - Variation around the mean = (data-mean)^2/n\n",
    "            - n is the sample size\n",
    "            - Var(Mean) = SS(Mean)/n\n",
    "            - Average sum of squares per mouse\n",
    "3. Sum up the squared resiguals around least-squares fit\n",
    "    - SS(fit) - sum of squares around the least squares fit.\n",
    "        - SS(fit) = (data-line)^2\n",
    "    - Variance around the fit is\n",
    "        - Var(Fit) = (data-line)^2/n\n",
    "        - Var(Fit) = SS(fit)/n\n",
    "        - Var(fit) = average ss(fit) for each mouse\n",
    "\n",
    "In general, Variance (something) = sums of squares / number of those things\n",
    "\n",
    "R-square = (Var(Mean) - Var(Fit)) / Var(Mean)\n",
    "Alternatively \n",
    "R-square = (SS(mean) - SS(fit))/SS(mean)\n",
    "Resulting value here can be explained x var explains % of y var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b93fa9",
   "metadata": {},
   "source": [
    "Concept basically is \n",
    "    - R-squared = (SS(mean) - SS(fit))/SS(mean)\n",
    "1. SS(Mean) = Measure, square, and sum the distance from the data to the mean.\n",
    "2. SS(fit) = Measure, square, and sum the distance from the data to the complicated equation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f27fcdc",
   "metadata": {},
   "source": [
    "p-value - statistical significance of r-squared\n",
    "    - number of more extreme values divided by all the values\n",
    "    - One F-Distribution = the degrees of freedom determine the shape\n",
    "    - p-value will be smaller when there are more samples relative to the number of parameters in the fit equation as the distribution tapers off faster\n",
    "\n",
    "F = (SS(mean) - SS(fit)) / (p(fit)-p(mean))\n",
    "        /SS(mean) / (n-p(fit))\n",
    "   \n",
    "    Where:  (p(fit)-p(mean)) and (n-p(fit)) are the degrees of freedom that turn the sums of squares into variances\n",
    "   \n",
    "    p(fit) is the number of parameters in the fit line\n",
    "        - y = y-intercept + slope x\n",
    "    p(mean) is the number of parameters in the mean line\n",
    "        - y = y-intercept or the mean value\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44ae22f",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "Linear regression\n",
    "1. Quantifies the relationship in the data (thi is the r-squared)\n",
    "    - This needs to be large\n",
    "2. Determines how reliable that relationship is (this is the p-value that we calculate with f)\n",
    "    - This needs to be small.\n",
    "\n",
    "Both should be achieved to have a good result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cd83cc",
   "metadata": {},
   "source": [
    "Understanding train/test split\n",
    "https://builtin.com/data-science/train-test-split   \n",
    "\n",
    "Train test split is a machine learning validation technique that divides the data into training and testing sets to simulate how models perform on new data. The method helps prevent overfitting and supports hyperparameter turning like max_depth\n",
    "\n",
    "Procedure:\n",
    "\n",
    "1. Arrange the data in a format acceptable for train test split. In scikit learn, this consists of separating your full data set in to 'Features' and 'Target'\n",
    "2. Split the data into two pieces, a training set and testing set.\n",
    "    - Need to indicate which goes where so for features it will be X and Y for target so it will become\n",
    "        - Training set:\n",
    "            X_train, Y_train\n",
    "        - Testing set\n",
    "            X_test, Y_test\n",
    "3. Train the model on the training set.\n",
    "4. Test the model on the testing set and evaluate the performance.\n",
    "\n",
    "\n",
    "Methods for splitting:\n",
    "1. Random splitting: randomly shuffling data and splitting it based on given percentages. This is also the default used in sklearn's train_test_split() method. \n",
    "    - Note that this is effective in large data sets but may not preserve class balance.\n",
    "2. Stratified splitting: Stratified splitting divides a data set in a way that preserves its proportion of classes or categories. This creates training and testing sets with class proportions representative of the original data set. Using stratified splitting can prevent model bias, and is most effective for imbalanced data sets or data sets where categories aren’t represented equally. In scikit-learn’s train_test_split() method, stratified splitting can be used by specifying the stratify parameter.\n",
    "\n",
    "3. Time-Based Splitting\n",
    "Time-based splitting involves organizing data in a set by points in time, ensuring past data is in the training set and future or later data is in the testing set. Splitting data based on time works to simulate real-world scenarios (for example, predicting future financial or market trends) and allows for time series analysis on time series data sets. However, one drawback to time-based splitting is that it may not fully capture trends for non-stationary data (data that continually changes over time). In scikit-learn, time series data can be split into training and testing sets by using the TimeSeriesSplit() method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-ai-ml-projects",
   "language": "python",
   "name": "ds-ai-ml-projects"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
