{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a88ae37",
   "metadata": {},
   "source": [
    "# Week 5 Notes\n",
    "### Primary Focus: Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9566fa5c",
   "metadata": {},
   "source": [
    "##### What is Feature Engineering?\n",
    "[Reference: Kaggle](https://www.kaggle.com/code/prashant111/a-reference-guide-to-feature-engineering-methods#1.-Introduction-to-Feature-Engineering-)\n",
    "\n",
    "**Feature Engineering** is the process of using domain knowledge to extract features from raw data via data mining techniques to improve the performance of machine earning algorithms.\n",
    "\n",
    "Coming up with features is difficult, time-consuming, and requires expert knowledge. \"Applied machine learning\" is basically feature engineering. If we can boil it down to one concept, it's about transforming raw data into a form that's more useful for your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2e7269",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "Models that rely on the **distance between data points** (like k-nearest neighbors or Support Vector Machines) are **highly sensitive to the scale of the features**. If one feature has a much larger range than others, it can dominate the distance calculations and skew the results.\n",
    "\n",
    "- **StandardScaler**\n",
    "    - This technique standardizes features by removing the mean and scaling to unit variance. \n",
    "    - The formula is z=(x−u)/s, where u is the mean and s is the standard deviation. It's great for features that follow a **normal or near-normal distribution**.\n",
    "    $$z=(x−u)/s$$\n",
    "- **MinMaxScaler**\n",
    "    -  This scales features to a fixed range, typically 0 to 1. \n",
    "    - The formula is Xscaled​ = (X−Xmin​)/(Xmax−Xmin). \n",
    "        $$X_{scaled} = \\frac{X - X_{min}}{X_{max} - X_{min}}$$\n",
    "    - This is useful when you need to **constrain your data to a specific range or when your data isn't normally distributed**.\n",
    "\n",
    " Scaling doesn't change the distribution of your data, it just changes the scale. This helps ensure all features contribute equally to the model.\n",
    "\n",
    "### Categorical Encoding \n",
    "Most machine learning algorithms require numerical input. Categorical features (like 'city' or 'product type') need to be converted to numbers before being used.\n",
    "- OneHotEncoder\n",
    "    - This is a common method for handling nominal (non-ordered) categorical data. It creates a new binary column for each category, with a **1 indicating the presence of that category and a 0 for its absence**. For example, if you have a 'city' feature with values 'New York', 'London', and 'Paris', OneHotEncoder would create three new columns: 'city_New York', 'city_London', and 'city_Paris'. This **prevents the model from assuming an arbitrary ordinal relationship between categories**.\n",
    "\n",
    "### Creating new features\n",
    "This is where the \"engineering\" comes in. By combining or transforming existing features, you can capture more complex relationships\n",
    "\n",
    "- **Polynomial features**\n",
    "    - You can create new features by raising existing features to a power e.g., $$x_1^2, x_2^2$$\n",
    "    - This **allows linear models to capture non-linear relationships**.\n",
    "- **Interaction terms**\n",
    "    - These are new features created by multiplying two or more existing features e.g., $$x_1 * x_2$$ \n",
    "    - This allows the model to capture the combined effect of two features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d8616d",
   "metadata": {},
   "source": [
    "Plan of study: \n",
    "- Go over each reference on Day 2 to 3 and implement on Day 4.\n",
    "\n",
    "References:\n",
    "- [Kaggle: A Reference Guide to Feature Engineering Methods](https://www.kaggle.com/code/prashant111/a-reference-guide-to-feature-engineering-methods)\n",
    "- [Geeks For Geeks: What is Feature Engineering?](https://www.geeksforgeeks.org/machine-learning/what-is-feature-engineering/)\n",
    "- [Datacamp: Feature Engineering in Machine Learning: A Practical Guide](https://www.datacamp.com/tutorial/feature-engineering)\n",
    "- [Practical Guide on Data Preprocessing in Python using Scikit Learn](https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#:~:text=It%20should%20be%20kept%20in,due%20to%20its%20larger%20range.)\n",
    "    - Downloaded the pdf here as it is required to login.\n",
    "- [Data Preprocessing with Scikit-learn](https://medium.com/@drpa/data-preprocessing-with-scikit-learn-dcaaf82d000a)\n",
    "- [10 Powerful Techniques for Feature Engineering in Machine Learning](https://thetechthinker.com/feature-engineering-in-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eec105",
   "metadata": {},
   "source": [
    "Additional items to study/read up on for later weeks:\n",
    "\n",
    "Can do this on the 5th or 6th day along with Cybersecurity this week since the activity is just a research. \n",
    "- Pipelines/orchestration tools (Airflow, Prefect, dbt)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
