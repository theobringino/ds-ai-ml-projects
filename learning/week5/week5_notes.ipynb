{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a88ae37",
   "metadata": {},
   "source": [
    "# Week 5 Notes\n",
    "### Primary Focus: Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9566fa5c",
   "metadata": {},
   "source": [
    "##### What is Feature Engineering?\n",
    "[Reference: Kaggle](https://www.kaggle.com/code/prashant111/a-reference-guide-to-feature-engineering-methods#1.-Introduction-to-Feature-Engineering-)\n",
    "\n",
    "**Feature Engineering** is the process of using domain knowledge to extract features from raw data via data mining techniques to improve the performance of machine earning algorithms.\n",
    "\n",
    "Coming up with features is difficult, time-consuming, and requires expert knowledge. \"Applied machine learning\" is basically feature engineering. - Andrew Ng\n",
    "\n",
    "If we can boil it down to one concept, it's about transforming raw data into a form that's more useful for your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2e7269",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "Models that rely on the **distance between data points** (like k-nearest neighbors or Support Vector Machines) are **highly sensitive to the scale of the features**. If one feature has a much larger range than others, it can dominate the distance calculations and skew the results.\n",
    "\n",
    "There will be other types discussed on other references.\n",
    "\n",
    "- **StandardScaler**\n",
    "    - This technique standardizes features by removing the mean and scaling to unit variance. \n",
    "    - The formula is z=(x−u)/s, where u is the mean and s is the standard deviation. It's great for features that follow a **normal or near-normal distribution**.\n",
    "    $$z=(x−u)/s$$\n",
    "- **MinMaxScaler**\n",
    "    -  This scales features to a fixed range, typically 0 to 1. \n",
    "    - The formula is Xscaled​ = (X−Xmin​)/(Xmax−Xmin). \n",
    "        $$X_{scaled} = \\frac{X - X_{min}}{X_{max} - X_{min}}$$\n",
    "    - This is useful when you need to **constrain your data to a specific range or when your data isn't normally distributed**.\n",
    "\n",
    " Scaling doesn't change the distribution of your data, it just changes the scale. This helps ensure all features contribute equally to the model.\n",
    "\n",
    "### Categorical Encoding \n",
    "Most machine learning algorithms require numerical input. Categorical features (like 'city' or 'product type') need to be converted to numbers before being used.\n",
    "- OneHotEncoder\n",
    "    - This is a common method for handling nominal (non-ordered) categorical data. It creates a new binary column for each category, with a **1 indicating the presence of that category and a 0 for its absence**. For example, if you have a 'city' feature with values 'New York', 'London', and 'Paris', OneHotEncoder would create three new columns: 'city_New York', 'city_London', and 'city_Paris'. This **prevents the model from assuming an arbitrary ordinal relationship between categories**.\n",
    "\n",
    "### Creating new features\n",
    "This is where the \"engineering\" comes in. By combining or transforming existing features, you can capture more complex relationships\n",
    "\n",
    "- **Polynomial features**\n",
    "    - You can create new features by raising existing features to a power e.g., $$x_1^2, x_2^2$$\n",
    "    - This **allows linear models to capture non-linear relationships**.\n",
    "- **Interaction terms**\n",
    "    - These are new features created by multiplying two or more existing features e.g., $$x_1 * x_2$$ \n",
    "    - This allows the model to capture the combined effect of two features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d8616d",
   "metadata": {},
   "source": [
    "Plan of study: \n",
    "- Go over each reference on Day 2 to 3 and implement on Day 4.\n",
    "\n",
    "References:\n",
    "- [Kaggle: A Reference Guide to Feature Engineering Methods](https://www.kaggle.com/code/prashant111/a-reference-guide-to-feature-engineering-methods)\n",
    "- [Geeks For Geeks: What is Feature Engineering?](https://www.geeksforgeeks.org/machine-learning/what-is-feature-engineering/)\n",
    "- [Datacamp: Feature Engineering in Machine Learning: A Practical Guide](https://www.datacamp.com/tutorial/feature-engineering)\n",
    "- [Practical Guide on Data Preprocessing in Python using Scikit Learn](https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#:~:text=It%20should%20be%20kept%20in,due%20to%20its%20larger%20range.)\n",
    "    - Downloaded the pdf here as it is required to login.\n",
    "- [Data Preprocessing with Scikit-learn](https://medium.com/@drpa/data-preprocessing-with-scikit-learn-dcaaf82d000a)\n",
    "- [10 Powerful Techniques for Feature Engineering in Machine Learning](https://thetechthinker.com/feature-engineering-in-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eec105",
   "metadata": {},
   "source": [
    "Additional items to study/read up on for later weeks:\n",
    "\n",
    "Can do this on the 5th or 6th day along with Cybersecurity this week since the activity is just a research. \n",
    "- Pipelines/orchestration tools (Airflow, Prefect, dbt)\n",
    "- Just a background here, no need for a deep dive. Add vids for tutorials if possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecb64dc",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4416ca",
   "metadata": {},
   "source": [
    "# Kaggle: A Reference Guide to Feature Engineering Methods\n",
    "[Kaggle: A Reference Guide to Feature Engineering Methods](https://www.kaggle.com/code/prashant111/a-reference-guide-to-feature-engineering-methods)\n",
    "\n",
    "Feature engineering is a very broad term that consists of different techniques to process data. These techniques help us to process our raw data into processed data ready to be fed into a machine learning algorithm. These techniques include filling missing values, encode categorical variables, variable transformation, create new variables from existing ones and others.\n",
    "\n",
    "There will be a separate Lab for this one under feature_engineering_kaggle.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab41684",
   "metadata": {},
   "source": [
    "This reference covers six items, namely:\n",
    "1. Missing data imputation\n",
    "2. Categorical Encoding\n",
    "3. Variable Transformation\n",
    "4. Discretization\n",
    "5. Outlier Engineering\n",
    "6. Date and Time Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952edb7c",
   "metadata": {},
   "source": [
    "## Missing Data Imputation\n",
    "\n",
    "- Missing data, or Missing values, occur when no data / no value is stored for a certain observation within a variable.\n",
    "- Missing data are a common occurrence and can have a significant effect on the conclusions that can be drawn from the data. \n",
    "- Incomplete data is an unavoidable problem in dealing with most data sources.\n",
    "- Imputation is the act of replacing missing data with statistical estimates of the missing values. \n",
    "- The goal of any imputation technique is to produce a complete dataset that can be used to train machine learning models.\n",
    "\n",
    "There are multiple techniques for missing data imputation. These are as follows:-\n",
    "1. Complete case analysis\n",
    "2. Mean / Median / Mode imputation\n",
    "3. Random Sample Imputation\n",
    "4. Replacement by Arbitrary Value\n",
    "5. End of Distribution Imputation\n",
    "6. Missing Value Indicator\n",
    "7. Multivariate imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423a3ce8",
   "metadata": {},
   "source": [
    "### Missing Data Mechanisms\n",
    "There are 3 mechanisms that lead to missing data and 2 of them involve missing data randomly or almost randomly with the third one caused by a systematic loss of data.\n",
    "\n",
    "1. Missing Completely at Random, MCAR\n",
    "\n",
    "    A variable is missing completely at random (MCAR) if the probability of being missing is the same for all the observations. When data is MCAR, there is absolutely no relationship between the data missing and any other values, observed or missing, within the dataset. In other words, those missing data points are a random subset of the data. There is nothing systematic going on that makes some data more likely to be missing than other.\n",
    "\n",
    "    If values for observations are missing completely at random, then disregarding those cases would not bias the inferences made.\n",
    "\n",
    "2. Missing at Random, MAR\n",
    "\n",
    "    MAR occurs when there is a systematic relationship between the propensity of missing values and the observed data. In other words, the probability an observation being missing depends only on available information (other variables in the dataset). For example, if men are more likely to disclose their weight than women, weight is MAR. The weight information will be missing at random for those men and women that decided not to disclose their weight, but as men are more prone to disclose it, there will be more missing values for women than for men.\n",
    "\n",
    "    In a situation like the above, if we decide to proceed with the variable with missing values (in this case weight), we might benefit from including gender to control the bias in weight for the missing observations.\n",
    "\n",
    "3. Missing Not at Random, MNAR\n",
    "    \n",
    "    Missing of values is not at random (MNAR) if their being missing depends on information not recorded in the dataset. In other words, there is a mechanism or a reason why missing values are introduced in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e6bd6f",
   "metadata": {},
   "source": [
    "**Noting here**\n",
    "\n",
    "From here on out, the lab will contain the sample codes and the notes will be in this document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d84674",
   "metadata": {},
   "source": [
    "### 1. Complete Case Analysis\n",
    "\n",
    "Complete case analysis implies analysing only those observations in the dataset that contain values in all the variables. In other words, in complete case analysis we remove all observations with missing values. This procedure is suitable when there are few observations with missing data in the dataset.\n",
    "\n",
    "So complete-case analysis (CCA), also called list-wise deletion of cases, consists in simply discarding observations where values in any of the variables are missing. Complete Case Analysis means literally analysing only those observations for which there is information in all of the variables (Xs).\n",
    "\n",
    "But, if the dataset contains missing data across multiple variables, or some variables contain a high proportion of missing observations, we can easily remove a big chunk of the dataset, and this is undesirable.\n",
    "\n",
    "CCA can be applied to both categorical and numerical variables.\n",
    "\n",
    "In practice, CCA may be an acceptable method when the amount of missing information is small. In many real life datasets, the amount of missing data is never small, and therefore CCA is typically never an option.\n",
    "\n",
    "So, in datasets with many variables that contain missing data, CCA will typically not be an option as it will produce a reduced dataset with complete observations. However, if only a subset of the variables from the dataset will be used, we could evaluate variable by variable, whether we choose to discard values with NA, or to replace them with other methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c0b5a9",
   "metadata": {},
   "source": [
    "### 2. Mean/Median/Mode Imputation\n",
    "\n",
    "We can replace missing values with the mean, median or mode of the variable. Mean / median / mode imputation is widely adopted in organisations and data competitions. Although in practice this technique is used in almost every situation, the procedure is suitable if data is missing at random and in small proportions. If there are a lot of missing observations, however, we will distort the distribution of the variable, as well as its relationship with other variables in the dataset. Distortion in the variable distribution may affect the performance of linear models.\n",
    "\n",
    "Mean/median imputation consists of replacing all occurrences of missing values (NA) within a variable by the mean (if the variable has a Gaussian distribution) or median (if the variable has a skewed distribution).\n",
    "\n",
    "For categorical variables, replacement by the mode, is also known as replacement by the most frequent category.\n",
    "\n",
    "Mean/median imputation has the assumption that the data are missing completely at random (MCAR). If this is the case, we can think of replacing the NA with the most frequent occurrence of the variable, which is the mean if the variable has a Gaussian distribution, or the median otherwise.\n",
    "\n",
    "The rationale is to replace the population of missing values with the most frequent value, since this is the most likely occurrence.\n",
    "\n",
    "When replacing NA with the mean or median, the variance of the variable will be distorted if the number of NA is big respect to the total number of observations (since the imputed values do not differ from the mean or from each other). Therefore leading to underestimation of the variance.\n",
    "\n",
    "In addition, estimates of covariance and correlations with other variables in the dataset may also be affected. This is because we may be destroying intrinsic correlations since the mean/median that now replace NA will not preserve the relation with the remaining variables\n",
    "\n",
    "Imputation should be done over the training set, and then propagated to the test set. This means that the mean/median to be used to fill missing values both in train and test set, should be extracted from the train set only. And this is to avoid overfitting.\n",
    "\n",
    "Mean/Median/Mode imputation is the most common method to impute missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c721de8",
   "metadata": {},
   "source": [
    "### 3. Random Sample Imputation\n",
    "\n",
    "Random sample imputation refers to randomly selecting values from the variable to replace the missing data. This technique preserves the variable distribution, and is well suited for data missing at random. But, we need to account for randomness by adequately setting a seed. Otherwise, the same missing observation could be replaced by different values in different code runs, and therefore lead to a different model predictions. This is not desirable when using our models within an organisation.\n",
    "\n",
    "Replacing of NA by random sampling for categorical variables is exactly the same as for numerical variables.\n",
    "\n",
    "Random sampling consist of taking a random observation from the pool of available observations of the variable, that is, from the pool of available categories, and using that randomly extracted value to fill the NA. In Random Sampling one takes as many random observations as missing values are present in the variable.\n",
    "\n",
    "By random sampling observations of the present categories, we guarantee that the frequency of the different categories/labels within the variable is preserved.\n",
    "\n",
    "Assumptions:\n",
    "Random sample imputation has the assumption that the data are missing completely at random (MCAR). If this is the case, it makes sense to substitute the missing values, by values extracted from the original variable distribution/ category frequency.\n",
    "\n",
    "\n",
    "Important Note\n",
    "Imputation should be done over the training set, and then propagated to the test set. This means that the random sample to be used to fill missing values both in train and test set, should be extracted from the train set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5c66d1",
   "metadata": {},
   "source": [
    "### 4. Replacement by Arbitrary Value\n",
    "\n",
    "Replacement by an arbitrary value, as its names indicates, refers to replacing missing data by any, arbitrarily determined value, but the same value for all missing data. Replacement by an arbitrary value is suitable if data is not missing at random, or if there is a huge proportion of missing values. If all values are positive, a typical replacement is -1. Alternatively, replacing by 999 or -999 are common practice. We need to anticipate that these arbitrary values are not a common occurrence in the variable. Replacement by arbitrary values however may not be suited for linear models, as it most likely will distort the distribution of the variables, and therefore model assumptions may not be met.\n",
    "For categorical variables, this is the equivalent of replacing missing observations with the label “Missing” which is a widely adopted procedure.\n",
    "\n",
    "Replacing the NA by artitrary values should be used when there are reasons to believe that the NA are not missing at random. In situations like this, we would not like to replace with the median or the mean, and therefore make the NA look like the majority of our observations.\n",
    "\n",
    "Instead, we want to flag them. We want to capture the missingness somehow.\n",
    "\n",
    "The arbitrary value has to be determined for each variable specifically.\n",
    "We can see that this is totally arbitrary. But, it is used in the industry. Typical values chosen by companies are -9999 or 9999, or similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc13094d",
   "metadata": {},
   "source": [
    "### 5. End of Distribution Imputation\n",
    "End of tail imputation involves replacing missing values by a value at the far end of the tail of the variable distribution. This technique is similar in essence to imputing by an arbitrary value. However, by placing the value at the end of the distribution, we need not look at each variable distribution individually, as the algorithm does it automatically for us. This imputation technique tends to work well with tree-based algorithms, but it may affect the performance of linear models, as it distorts the variable distribution.\n",
    "\n",
    "On occasions, one has reasons to suspect that missing values are not missing at random. And if the value is missing, there has to be a reason for it. Therefore, we would like to capture this information.\n",
    "\n",
    "Adding an additional variable indicating missingness may help with this task. However, the values are still missing in the original variable, and they need to be replaced if we plan to use the variable in machine learning.\n",
    "\n",
    "So, we will replace the NA, by values that are at the far end of the distribution of the variable.\n",
    "\n",
    "The rationale is that if the value is missing, it has to be for a reason, therefore, we would not like to replace missing values for the mean and make that observation look like the majority of our observations. Instead, we want to flag that observation as different, and therefore we assign a value that is at the tail of the distribution, where observations are rarely represented in the population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7600e21",
   "metadata": {},
   "source": [
    "### 6. Missing Value Indicator\n",
    "\n",
    "The missing indicator technique involves adding a binary variable to indicate whether the value is missing for a certain observation. This variable takes the value 1 if the observation is missing, or 0 otherwise. One thing to notice is that we still need to replace the missing values in the original variable, which we tend to do with mean or median imputation. By using these 2 techniques together, if the missing value has predictive power, it will be captured by the missing indicator, and if it doesn’t it will be masked by the mean / median imputation.\n",
    "\n",
    "These 2 techniques in combination tend to work well with linear models. But, adding a missing indicator expands the feature space and, as multiple variables tend to have missing values for the same observations, many of these newly created binary variables could be identical or highly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41294c69",
   "metadata": {},
   "source": [
    "### Conclusion - When to use each imputation method?\n",
    "If missing values are less than 5% of the variable, then go for mean/median imputation or random sample replacement. Impute by most frequent category if missing values are more than 5% of the variable. Do mean/median imputation+adding an additional binary variable to capture missingness add a 'Missing' label in categorical variables.\n",
    "\n",
    "If the number of NA in a variable is small, they are unlikely to have a strong impact on the variable / target that you are trying to predict. Therefore, treating them specially, will most certainly add noise to the variables. Therefore, it is more useful to replace by mean/random sample to preserve the variable distribution.\n",
    "\n",
    "If the variable / target you are trying to predict is however highly unbalanced, then it might be the case that this small number of NA are indeed informative.\n",
    "\n",
    "#### Exceptions\n",
    "If we suspect that NAs are not missing at random and do not want to attribute the most common occurrence to NA, and if we don't want to increase the feature space by adding an additional variable to indicate missingness - in these cases, replace by a value at the far end of the distribution or an arbitrary value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5e8b8f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6d357f",
   "metadata": {},
   "source": [
    "## Categorical Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316647e4",
   "metadata": {},
   "source": [
    "Categorical data is data that takes only a limited number of values.\n",
    "\n",
    "For example, if you people responded to a survey about which what brand of car they owned, the result would be categorical (because the answers would be things like Honda, Toyota, Ford, None, etc.). Responses fall into a fixed set of categories.\n",
    "\n",
    "You will get an error if you try to plug these variables into most machine learning models in Python without \"encoding\" them first. Here we'll show the most popular method for encoding categorical variables.\n",
    "\n",
    "Categorical variable encoding is a broad term for collective techniques used to transform the strings or labels of categorical variables into numbers. There are multiple techniques under this method:\n",
    "\n",
    "1. One-Hot encoding (OHE)\n",
    "2. Ordinal encoding\n",
    "3. Count and Frequency encoding\n",
    "4. Target encoding / Mean encoding\n",
    "5. Weight of Evidence\n",
    "6. Rare label encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ced773",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding (OHE)\n",
    "OHE is the standard approach to encode categorical data.\n",
    "\n",
    "One hot encoding (OHE) creates a binary variable for each one of the different categories present in a variable. These binary variables take 1 if the observation shows a certain category or 0 otherwise. OHE is suitable for linear models. But, OHE expands the feature space quite dramatically if the categorical variables are highly cardinal, or if there are many categorical variables. In addition, many of the derived dummy variables could be highly correlated.\n",
    "\n",
    "OHE, consists of replacing the categorical variable by different boolean variables, which take value 0 or 1, to indicate whether or not a certain category / label of the variable was present for that observation. Each one of the boolean variables are also known as dummy variables or binary variables.\n",
    "\n",
    "For example, from the categorical variable \"Gender\", with labels 'female' and 'male', we can generate the boolean variable \"female\", which takes 1 if the person is female or 0 otherwise. We can also generate the variable male, which takes 1 if the person is \"male\" and 0 otherwise.\n",
    "\n",
    "Note that for categorical variables that only have 2 categories, we only need 1 dummy variable here. Ex. Gender, could be Male or Female, if Male then 1 else 0 thus Female or vice versa: If Female then 1 else 0 thus Male.\n",
    "\n",
    "Scikt-Learn API provides a class for [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html).\n",
    "\n",
    "Important Note:\n",
    "Scikit-learn's one hot encoder class only takes numerical categorical values. So, any value of string type should be label encoded first before one hot encoded.\n",
    "\n",
    "In the titanic example, the gender of the passengers has to be label encoded first before being one-hot encoded using Scikit-learn's one hot encoder class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d19fb28",
   "metadata": {},
   "source": [
    "#### Ordinal Encoding\n",
    "\n",
    "Categorical variable which categories can be meaningfully ordered are called ordinal. For example:\n",
    "\n",
    "- Student's grade in an exam (A, B, C or Fail).\n",
    "- Days of the week can be ordinal with Monday = 1, and Sunday = 7.\n",
    "Educational level, with the categories: Elementary school, High school, College graduate, PhD ranked from 1 to 4.\n",
    "- When the categorical variable is ordinal, the most straightforward approach is to replace the labels by some ordinal number.\n",
    "\n",
    "In ordinal encoding we replace the categories by digits, either arbitrarily or in an informed manner. If we encode categories arbitrarily, we assign an integer per category from 1 to n, where n is the number of unique categories. If instead, we assign the integers in an informed manner, we observe the target distribution: we order the categories from 1 to n, assigning 1 to the category for which the observations show the highest mean of target value, and n to the category with the lowest target mean value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77384ab1",
   "metadata": {},
   "source": [
    "#### Count and Frequency Encoding \n",
    "\n",
    "In count encoding we replace the categories by the count of the observations that show that category in the dataset. Similarly, we can replace the category by the frequency -or percentage- of observations in the dataset. That is, if 10 of our 100 observations show the colour blue, we would replace blue by 10 if doing count encoding, or by 0.1 if replacing by the frequency. These techniques capture the representation of each label in a dataset, but the encoding may not necessarily be predictive of the outcome.\n",
    "\n",
    "This approach is heavily used in Kaggle competitions, wherein we replace each label of the categorical variable by the count, this is the amount of times each label appears in the dataset. Or the frequency, this is the percentage of observations within that category. The two methods are equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354e78b6",
   "metadata": {},
   "source": [
    "#### Target/Mean Encoding\n",
    "\n",
    "In target encoding, also called mean encoding, we replace each category of a variable, by the mean value of the target for the observations that show a certain category. For example, we have the categorical variable “city”, and we want to predict if the customer will buy a TV provided we send a letter. If 30 percent of the people in the city “London” buy the TV, we would replace London by 0.3.\n",
    "\n",
    "This technique has 3 advantages:\n",
    "- it does not expand the feature space,\n",
    "- it captures some information regarding the target at the time of encoding the category, and\n",
    "- it creates a monotonic relationship between the variable and the target.\n",
    "\n",
    "Monotonic relationships between variable and target tend to improve linear model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44beb347",
   "metadata": {},
   "source": [
    "#### Weight of Evidence\n",
    "Weight of evidence (WOE) is a technique used to encode categorical variables for classification. WOE is the natural logarithm of the probability of the target being 1 divided the probability of the target being 0. WOE has the property that its value will be 0 if the phenomenon is random; it will be bigger than 0 if the probability of the target being 0 is bigger, and it will be smaller than 0 when the probability of the target being 1 is greater.\n",
    "\n",
    "WOE transformation creates a nice visual representation of the variable, because by looking at the WOE encoded variable, we can see, category by category, whether it favours the outcome of 0, or of 1. In addition, WOE creates a monotonic relationship between variable and target, and leaves all the variables within the same value range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1004ca",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9889c2f9",
   "metadata": {},
   "source": [
    "## Variable Transformation\n",
    "Some machine learning models like linear and logistic regression assume that the variables are normally distributed. Others benefit from Gaussian-like distributions, as in such distributions the observations of X available to predict Y vary across a greater range of values. Thus, Gaussian distributed variables may boost the machine learning algorithm performance.\n",
    "\n",
    "If a variable is not normally distributed, sometimes it is possible to find a mathematical transformation so that the transformed variable is Gaussian. Typically used mathematical transformations are:\n",
    "\n",
    "1. Logarithm transformation - $$log(x)$$\n",
    "2. Reciprocal transformation - $$1 / x$$\n",
    "3. Square root transformation - $$sqrt(x)$$\n",
    "4. Exponential transformation - $$exp(x)$$\n",
    "5. Box-Cox transformation\n",
    "\n",
    "Refer to the lab to see how these are transformed, and expound here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee076551",
   "metadata": {},
   "source": [
    "#### BoxCox Transformation\n",
    "\n",
    "The Box-Cox transformation is defined as:\n",
    "\n",
    "$$ T(Y)=(Y exp(λ)−1)/λ $$\n",
    "\n",
    "where: \n",
    "- Y is the response variable, and \n",
    "- λ is the transformation parameter. \n",
    "\n",
    "λ varies from -5 to 5. In the transformation, all values of λ are considered and the optimal value for a given variable is selected.\n",
    "\n",
    "Briefly, for each λ (the transformation tests several λs), the correlation coefficient of the Probability Plot (Q-Q plot below, correlation between ordered values and theoretical quantiles) is calculated.\n",
    "\n",
    "The value of λ corresponding to the maximum correlation on the plot is then the optimal choice for λ.\n",
    "\n",
    "In python, we can evaluate and obtain the best λ with the stats.boxcox function from the package scipy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6274e1d0",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c87b338",
   "metadata": {},
   "source": [
    "## Discretization\n",
    "\n",
    "- Discretisation is the process of transforming continuous variables into discrete variables by creating a set of contiguous intervals that spans the range of the variable's values.\n",
    "- Discretisation helps handle outliers and highly skewed variables\n",
    "- Discretisation helps handle outliers by placing these values into the lower or higher intervals together with the remaining inlier values of the distribution. Thus, these outlier observations no longer differ from the rest of the values at the tails of the distribution, as they are now all together in the same interval / bucket. In addition, by creating appropriate bins or intervals, discretisation can help spread the values of a skewed variable across a set of bins with equal number of observations.\n",
    "\n",
    "There are several approaches to transform continuous variables into discrete ones. This process is also known as binning, with each bin being each interval.\n",
    "\n",
    "Discretisation refers to sorting the values of the variable into bins or intervals, also called buckets. There are multiple ways to discretise variables:\n",
    "1. Equal width discretisation\n",
    "2. Equal Frequency discretisation\n",
    "3. Domain knowledge discretisation\n",
    "4. Discretisation using decision trees\n",
    "\n",
    "\n",
    "**Discretising data with pandas cut and qcut functions**\n",
    "\n",
    "\n",
    "When dealing with continuous numeric data, it is often helpful to bin the data into multiple buckets for further analysis. Pandas supports these approaches using the cut and qcut functions.\n",
    "\n",
    "cut command creates equispaced bins but frequency of samples is unequal in each bin.\n",
    "\n",
    "qcut command creates unequal size bins but frequency of samples is equal in each bin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d9936d",
   "metadata": {},
   "source": [
    "#### Equal width discretisation with pandas cut function\n",
    "\n",
    "Equal width binning divides the scope of possible values into N bins of the same width.The width is determined by the range of values in the variable and the number of bins we wish to use to divide the variable.\n",
    "\n",
    "$$width = (max value - min value) / N$$\n",
    "\n",
    "For example if the values of the variable vary between 0 and 100, we create 5 bins like this: width = (100-0) / 5 = 20. The bins thus are 0-20, 20-40, 40-60, 80-100. The first and final bins (0-20 and 80-100) can be expanded to accommodate outliers (that is, values under 0 or greater than 100 would be placed in those bins as well).\n",
    "\n",
    "There is no rule of thumb to define N. Typically, we would not want more than 10.\n",
    "\n",
    "Source : https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.cut.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e347e8",
   "metadata": {},
   "source": [
    "#### Equal frequency discretisation with pandas qcut function\n",
    "\n",
    "Equal frequency binning divides the scope of possible values of the variable into N bins, where each bin carries the same amount of observations. This is particularly useful for skewed variables as it spreads the observations over the different bins equally. Typically, we find the interval boundaries by determining the quantiles.\n",
    "\n",
    "Equal frequency discretisation using quantiles consists of dividing the continuous variable into N quantiles, N to be defined by the user. There is no rule of thumb to define N. However, if we think of the discrete variable as a categorical variable, where each bin is a category, we would like to keep N (the number of categories) low (typically no more than 10).\n",
    "\n",
    "Source : https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.qcut.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4761836b",
   "metadata": {},
   "source": [
    "#### Domain knowledge discretisation\n",
    "\n",
    "Frequently, when engineering variables in a business setting, the business experts determine the intervals in which they think the variable should be divided so that it makes sense for the business. These intervals may be defined both arbitrarily or following some criteria of use to the business. Typical examples are the discretisation of variables like Age and Income.\n",
    "\n",
    "Income for example is usually capped at a certain maximum value, and all incomes above that value fall into the last bucket. As per Age, it is usually divided in certain groups according to the business need, for example division into 0-21 (for under-aged), 20-30 (for young adults), 30-40, 40-60, and > 60 (for retired or close to) are frequent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ca0d3a",
   "metadata": {},
   "source": [
    "## Outlier Engineering\n",
    "\n",
    "Outliers are values that are unusually high or unusually low respect to the rest of the observations of the variable. There are a few techniques for outlier handling:\n",
    "- Outlier removal\n",
    "- Treating outliers as missing values\n",
    "- Discretisation\n",
    "- Top / bottom / zero coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2ee817",
   "metadata": {},
   "source": [
    "#### Identifying outliers\n",
    "\n",
    "Extreme Value Analysis\n",
    "The most basic form of outlier detection is Extreme Value Analysis of 1-dimensional data. The key for this method is to determine the statistical tails of the underlying distribution of the variable, and then finding the values that sit at the very end of the tails.\n",
    "\n",
    "In the typical scenario, the distribution of the variable is Gaussian and thus outliers will lie outside the mean plus or minus 3 times the standard deviation of the variable.\n",
    "\n",
    "If the variable is not normally distributed, a general approach is to calculate the quantiles, and then the interquantile range (IQR), as follows:\n",
    "- IQR = 75th quantile - 25th quantile\n",
    "\n",
    "An outlier will sit outside the following upper and lower boundaries:\n",
    "- Upper boundary = 75th quantile + (IQR * 1.5)\n",
    "- Lower boundary = 25th quantile - (IQR * 1.5)\n",
    "\n",
    "or for extreme cases:\n",
    "- Upper boundary = 75th quantile + (IQR * 3)\n",
    "- Lower boundary = 25th quantile - (IQR * 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead716df",
   "metadata": {},
   "source": [
    "#### Outlier Removal\n",
    "\n",
    "Outlier removal refers to removing outlier observations from the dataset. Outliers, by nature are not abundant, so this procedure should not distort the dataset dramatically. But if there are outliers across multiple variables, we may end up removing a big portion of the dataset.\n",
    "\n",
    "#### Treating outliers as missing values\n",
    "We can treat outliers as missing information, and carry on any of the imputation methods described earlier in this kernel\n",
    "\n",
    "#### Discretization\n",
    "Discretisation handles outliers automatically, as outliers are sorted into the terminal bins, together with the other higher or lower value observations. The best approaches are equal frequency and tree based discretisation.\n",
    "\n",
    "#### Top/Bottom/Zero Coding\n",
    "Top or bottom coding are also known as Winsorisation or outlier capping. The procedure involves capping the maximum and minimum values at a predefined value. This predefined value can be arbitrary, or it can be derived from the variable distribution.\n",
    "\n",
    "If the variable is normally distributed we can cap the maximum and minimum values at the mean plus or minus 3 times the standard deviation. If the variable is skewed, we can use the inter-quantile range proximity rule or cap at the top and bottom percentiles.\n",
    "\n",
    "**Top-coding important**\n",
    "\n",
    "Top-coding and bottom-coding, as any other feature pre-processing step, should be determined over the training set, and then transferred onto the test set. This means that we should find the upper and lower bounds in the training set only, and use those bands to cap the values in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7d3918",
   "metadata": {},
   "source": [
    "## Date and Time Engineering\n",
    "Date variables are special type of categorical variable. By their own nature, date variables will contain a multitude of different labels, each one corresponding to a specific date and sometimes time. Date variables, when preprocessed properly can highly enrich a dataset. For example, from a date variable we can extract:\n",
    "\n",
    "- Month\n",
    "- Quarter\n",
    "- Semester\n",
    "- Day (number)\n",
    "- Day of the week\n",
    "- Is Weekend?\n",
    "- Hr\n",
    "- Time differences in years, months, days, hrs, etc.\n",
    "\n",
    "It is important to understand that date variables should not be used as the categorical variables we have been working so far when building a machine learning model. Not only because they have a multitude of categories, but also because when we actually use the model to score a new observation, this observation will most likely be in the future, an therefore its date label, will be different than the ones contained in the training set and therefore the ones used to train the machine learning algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14c0095",
   "metadata": {},
   "source": [
    "# Geeks for Geeks: Feature Engineering Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a621676d",
   "metadata": {},
   "source": [
    "Importance of Feature Engineering\n",
    "Feature engineering can significantly influence model performance. By refining features, we can:\n",
    "\n",
    "- Improve accuracy: Choosing the right features helps the model learn better, leading to more accurate predictions.\n",
    "- Reduce overfitting: Using fewer, more important features helps the model avoid memorizing the data and perform better on new data.\n",
    "- Boost interpretability: Well-chosen features make it easier to understand how the model makes its predictions.\n",
    "- Enhance efficiency: Focusing on key features speeds up the model’s training and prediction process, saving time and resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6f4b58",
   "metadata": {},
   "source": [
    "### Processes Involved in Feature Engineering: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbcd8be",
   "metadata": {},
   "source": [
    "1. Feature Creation: Feature creation involves generating new features from domain knowledge or by observing patterns in the data. It can be:\n",
    "    - Domain-specific: Created based on industry knowledge like business rules.\n",
    "    - Data-driven: Derived by recognizing patterns in data.\n",
    "    - Synthetic: Formed by combining existing features.\n",
    "2. Feature Transformation: Transformation adjusts features to improve model learning:\n",
    "    - Normalization & Scaling: Adjust the range of features for consistency.\n",
    "    - Encoding: Converts categorical data to numerical form i.e one-hot encoding.\n",
    "    - Mathematical transformations: Like logarithmic transformations for skewed data.\n",
    "3. Feature Extraction: Extracting meaningful features can reduce dimensionality and improve model accuracy:\n",
    "    - Dimensionality reduction: Techniques like PCA reduce features while preserving important information.\n",
    "    - Aggregation & Combination: Summing or averaging features to simplify the model.\n",
    "4. Feature Selection: Feature selection involves choosing a subset of relevant features to use:\n",
    "    - Filter methods: Based on statistical measures like correlation.\n",
    "    - Wrapper methods: Select based on model performance.\n",
    "    - Embedded methods: Feature selection integrated within model training.\n",
    "5. Feature Scaling: Scaling ensures that all features contribute equally to the model:\n",
    "    - Min-Max scaling: Rescales values to a fixed range like 0 to 1.\n",
    "    - Standard scaling: Normalizes to have a mean of 0 and variance of 1.\n",
    "\n",
    "## Steps in Feature Engineering\n",
    "Feature engineering can vary depending on the specific problem but the general steps are:\n",
    "\n",
    "1. Data Cleaning: Identify and correct errors or inconsistencies in the dataset to ensure data quality and reliability.\n",
    "2. Data Transformation: Transform raw data into a format suitable for modeling including scaling, normalization and encoding.\n",
    "3. Feature Extraction: Create new features by combining or deriving information from existing ones to provide more meaningful input to the model.\n",
    "4. Feature Selection: Choose the most relevant features for the model using techniques like correlation analysis, mutual information and stepwise regression.\n",
    "5. Feature Iteration: Continuously refine features based on model performance by adding, removing or modifying features for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c605b19b",
   "metadata": {},
   "source": [
    "### Common Techniques in Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d694f311",
   "metadata": {},
   "source": [
    "1. **One-Hot Encoding**: One-Hot Encoding converts categorical variables into binary indicators, allowing them to be used by machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "420fecc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Color_Blue  Color_Green  Color_Red\n",
      "0       False        False       True\n",
      "1        True        False      False\n",
      "2       False         True      False\n",
      "3        True        False      False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'Color': ['Red', 'Blue', 'Green', 'Blue']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df_encoded = pd.get_dummies(df, columns=['Color'], prefix='Color')\n",
    "\n",
    "print(df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1a2dd6",
   "metadata": {},
   "source": [
    "2. **Binning**: Binning transforms continuous variables into discrete bins, making them categorical for easier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b03c94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age Age_Group\n",
      "0   23     21-40\n",
      "1   45     41-60\n",
      "2   18      0-20\n",
      "3   34     21-40\n",
      "4   67       61+\n",
      "5   50     41-60\n",
      "6   21     21-40\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'Age': [23, 45, 18, 34, 67, 50, 21]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "bins = [0, 20, 40, 60, 100]\n",
    "labels = ['0-20', '21-40', '41-60', '61+']\n",
    "\n",
    "df['Age_Group'] = pd.cut(df['Age'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b1ceec",
   "metadata": {},
   "source": [
    "3. **Text Data Preprocessing**: Involves removing stop-words, stemming and vectorizing text data to prepare it for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c1c29bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to D:/nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# install corpus\n",
    "import nltk\n",
    "nltk.download('stopwords', download_dir='D:/nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da038839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Texts: ['sampl sentence.', 'text data preprocess important.']\n",
      "Vectorized Text: [[0 0 0 1 1 0]\n",
      " [1 1 1 0 0 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Bennywutzsx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "texts = [\"This is a sample sentence.\", \"Text data preprocessing is important.\"]\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    words = text.split()\n",
    "    words = [stemmer.stem(word)\n",
    "             for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "cleaned_texts = [preprocess_text(text) for text in texts]\n",
    "\n",
    "X = vectorizer.fit_transform(cleaned_texts)\n",
    "\n",
    "print(\"Cleaned Texts:\", cleaned_texts)\n",
    "print(\"Vectorized Text:\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56035afa",
   "metadata": {},
   "source": [
    "4. **Feature Splitting**: Divides a single feature into multiple sub-features, uncovering valuable insights and improving model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "962fc613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Full_Address      Street         City Zipcode\n",
      "0  123 Elm St, Springfield, 12345  123 Elm St  Springfield   12345\n",
      "1  456 Oak Rd, Shelbyville, 67890  456 Oak Rd  Shelbyville   67890\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'Full_Address': [\n",
    "    '123 Elm St, Springfield, 12345', '456 Oak Rd, Shelbyville, 67890']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df[['Street', 'City', 'Zipcode']] = df['Full_Address'].str.extract(\n",
    "    r'([0-9]+\\s[\\w\\s]+),\\s([\\w\\s]+),\\s(\\d+)')\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8b37c5",
   "metadata": {},
   "source": [
    "Tools for Feature Engineering\n",
    "There are several tools available for feature engineering. Here are some popular ones:\n",
    "\n",
    "- Featuretools: Automates feature engineering by extracting and transforming features from structured data. It integrates well with libraries like pandas and scikit-learn making it easy to create complex features without extensive coding.\n",
    "- TPOT: Uses genetic algorithms to optimize machine learning pipelines, automating feature selection and model optimization. It visualizes the entire process, helping you identify the best combination of features and algorithms.\n",
    "- DataRobot: Automates machine learning workflows including feature engineering, model selection and optimization. It supports time-dependent and text data and offers collaborative tools for teams to efficiently work on projects.\n",
    "- Alteryx: Offers a visual interface for building data workflows, simplifying feature extraction, transformation and cleaning. It integrates with popular data sources and its drag-and-drop interface makes it accessible for non-programmers.\n",
    "- H2O.ai: Provides both automated and manual feature engineering tools for a variety of data types. It includes features for scaling, imputation and encoding and offers interactive visualizations to better understand model results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-ai-ml-projects",
   "language": "python",
   "name": "ds-ai-ml-projects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
