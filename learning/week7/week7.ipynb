{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d356c855",
   "metadata": {},
   "source": [
    "# Week 8: Advanced Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984bbb5c",
   "metadata": {},
   "source": [
    "Complex regression types like Polynomial, Ridge, Lasso, Elastic Net, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44adee81",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/machine-learning/ml-different-regression-types/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe70192e",
   "metadata": {},
   "source": [
    "Regression Analysis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Regularization Techniques\n",
    "\n",
    "Types of Regression in ML\n",
    "\n",
    "Linear\n",
    "\n",
    "Logistic\n",
    "\n",
    "Polynomial\n",
    "\n",
    "Softmax Regression\n",
    "\n",
    "Ridge Regression\n",
    "\n",
    "Lasso Regression \n",
    "\n",
    "Elastic Net Regression\n",
    "\n",
    "Need for Regression\n",
    "\n",
    "Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7de378",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/machine-learning/ml-linear-regression/\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2022/01/different-types-of-regression-models/\n",
    "\n",
    "https://scikit-learn.org/stable/modules/linear_model.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60eab64",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664adb8a",
   "metadata": {},
   "source": [
    "#### Polynomial Regression\n",
    "\n",
    "Concept: Understanding how to model non-linear relationships by introducing polynomial features $$x^2, x^3$$\n",
    "\n",
    "Key Challenge: The risk of Overfitting with high-degree polynomials.\n",
    "\n",
    "What if our data is actually more complex than a simple straight line? Surprisingly, we can actually use a linear model to fit nonlinear data. A simple way to do this is to add powers of each feature as new features, then train a linear model on this extended set of features. This technique is called Polynomial Regression.The equation below represents a polynomial equation:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\cdots + \\beta_n x^n + \\varepsilon.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1252e32",
   "metadata": {},
   "source": [
    "### Regularized Regression \n",
    "Concept: Introduction to the concept of a penalty term or regularizer added to the loss function to constrain model complexity.\n",
    "\n",
    "##### Ridge Regression \n",
    "$$L_2 Regularization$$\n",
    "\n",
    " Rigid Regression is a regularized version of Linear Regression where a regularized term is added to the cost function. This forces the learning algorithm to not only fit the data but also keep the model weights as small as possible. Note that the regularized term should not be added to the cost function during training. Once the model is trained, you want to evaluate the model's performance using the unregularized performance measure. The formula for ridge regression is:\n",
    "$$J(\\mathbf{\\theta}) = \\text{MSE}(\\mathbf{\\theta}) + \\alpha \\frac{1}{2}\\sum_{i=1}^{n} \\theta_i^2$$\n",
    "\n",
    "Explanation of the Formula Components:\n",
    "\n",
    "* $J(\\mathbf{\\theta})$: The **Cost Function** (or loss function) that the model minimizes.\n",
    "* $\\text{MSE}(\\mathbf{\\theta})$: The **Mean Squared Error** component, which measures the model's performance on the training data. This is the standard part of the cost function for linear regression.\n",
    "* $\\alpha \\frac{1}{2}\\sum_{i=1}^{n} \\theta_i^2$: The **Regularization Term** (or penalty term).\n",
    "    * $\\mathbf{\\theta}$: The vector of model **parameters** (the weights or coefficients).\n",
    "    * $\\theta_i$: The $i$-th **parameter** (excluding the intercept, $\\theta_0$).\n",
    "    * $\\sum_{i=1}^{n} \\theta_i^2$: The sum of the squared parameters (**L2-norm**).\n",
    "    * $\\alpha$ (alpha): The **regularization hyperparameter** that controls the strength of the penalty. A larger $\\alpha$ forces the model to use smaller weights.\n",
    "    * $\\frac{1}{2}$: A constant factor often included for mathematical convenience, ensuring that the derivative of the term is simply $\\alpha\\sum \\theta_i$.\n",
    "\n",
    "Ridge Regression, also known as **L2 Regularization**, adds this penalty term to prevent **overfitting** by keeping the model weights small.\n",
    "\n",
    "- Adds the squared magnitude of coefficients to the loss function.\n",
    "- Shrinks coefficients towards zero, reducing variance.\n",
    "- Supplementary: The λ (or α) hyperparameter and its role."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49f403a",
   "metadata": {},
   "source": [
    "#### Lasso Regression \n",
    "$$L_1 ​Regularization$$\n",
    "\n",
    " Similar to Ridge Regression, Lasso (Least Absolute Shrinkage and Selection Operator) is another regularized version of Linear regression : it adds a regularized term to the cost function, but it uses the l1 norm of the weighted vector instead of half the square of the l2 term Lasso regression is given as:\n",
    "\n",
    "$$J(\\mathbf{\\theta}) = \\text{MSE}(\\mathbf{\\theta}) + \\alpha \\sum_{i=1}^{n} |\\theta_i|$$\n",
    "\n",
    "Explanation of the Formula Components\n",
    "\n",
    "This equation is very similar to the Ridge Regression cost function, but it uses the **absolute value of the weights** instead of the square, which has a different effect on the model.\n",
    "\n",
    "* $J(\\mathbf{\\theta})$: The **Cost Function** that the model minimizes.\n",
    "* $\\text{MSE}(\\mathbf{\\theta})$: The **Mean Squared Error** component (the loss from prediction accuracy).\n",
    "* $\\alpha \\sum_{i=1}^{n} |\\theta_i|$: The **Regularization Term** (**L1 Penalty**).\n",
    "    * $\\mathbf{\\theta}$: The vector of model **parameters** (weights).\n",
    "    * $\\sum_{i=1}^{n} |\\theta_i|$: The sum of the absolute values of the parameters (**L1-norm**).\n",
    "    * $\\alpha$ (alpha): The **regularization hyperparameter**.\n",
    "\n",
    "The key distinction of Lasso ($\\sum |\\theta_i|$) is its ability to drive the coefficients of unimportant features to **exactly zero**, effectively performing **feature selection**.\n",
    "\n",
    "- Adds the absolute magnitude of coefficients to the loss function.\n",
    "- Can drive some coefficients exactly to zero, effectively performing Feature Selection.\n",
    "- Supplementary: The λ (or α) hyperparameter and its role.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf80aa76",
   "metadata": {},
   "source": [
    "## Regularization Comparison: L2 vs. L1\n",
    "\n",
    "| Feature | L2 (Ridge Regression) | L1 (Lasso Regression) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Penalty Term** | Sum of **squared** coefficients ($\\sum \\theta_i^2$). | Sum of the **absolute values** of coefficients ($\\sum |\\theta_i|$). |\n",
    "| **Effect on Coefficients** | **Drives coefficients down** (shrinks them) toward zero. | **Drives coefficients of unimportant features to *exactly* zero.** |\n",
    "| **Primary Benefit** | Reduces variance and prevents **overfitting** by keeping all model weights small. | Performs **automatic feature selection** by eliminating unimportant features. |\n",
    "| **Outcome** | All original features remain in the model, but with reduced influence. | The resulting model is simpler and more interpretable as it only includes the most relevant features. |\n",
    "\n",
    "***\n",
    "### Key Takeaway on Magnitude and Prediction\n",
    "\n",
    "> \"so the more magnitude a feature has, the more it will be felt and the less or close to 0, it will not affect the prediction\"\n",
    "\n",
    "* **L2 (Ridge):** Penalizes *large* coefficients heavily because the penalty is proportional to $\\theta_i^2$. It forces large coefficients to shrink, meaning that a feature with a very high magnitude (influence) will have its contribution to the prediction ***reduced***.\n",
    "* **L1 (Lasso):** Also penalizes large coefficients, but its unique geometry allows it to push coefficients for irrelevant features **all the way to zero** instead of just close to zero. This means that features driven to $0$ truly have **zero effect** on the prediction, fulfilling the goal of **feature selection**.\n",
    "\n",
    "Basically:\n",
    "\n",
    "- L2 (Ridge Regression): Correctly drives coefficients down or reduces their magnitude using a penalty term to penalize large weights. This keeps the overall model weights small to prevent overfitting.\n",
    "\n",
    "- L1 (Lasso Regression): Correctly drives coefficients of unimportant features to 0 (exactly zero) using its penalty term, which allows it to be used for feature selection.\n",
    "\n",
    "the more magnitude a coefficient has, the more the feature will influence the prediction. L1 and L2 reduce this influence, but L1 goes the extra step of setting it to zero for useless features.\n",
    "\n",
    "- Coefficient≈0⟹No/Minimal effect on prediction.\n",
    "\n",
    "- Coefficient=0⟹Zero effect on prediction (Lasso’s unique ability)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b878d5",
   "metadata": {},
   "source": [
    "#### Role of hyperparameter lambda and alpha \n",
    "\n",
    "The hyperparameter λ (lambda), often represented as α (alpha) in practical implementations like Python's scikit-learn, is the single most important control mechanism in both L1 (Lasso) and L2 (Ridge) regularization. It controls the strength of the penalty applied to the model's coefficients.\n",
    "\n",
    "The hyperparameter λ (or α) controls the strength of the penalty term added to the loss function. This penalty manages the bias-variance trade-off to ensure the model generalizes well, mitigating the risks of both overfitting (too complex) and underfitting (too simple).\n",
    "\n",
    "##### Role of λ / α in Regularization\n",
    "$$Cost=Loss(Data Fit)+λ×Penalty(Model Complexity)$$\n",
    "\n",
    "The λ or α value is the constant that multiplies the penalty term, determining the trade-off between:\n",
    "1. Fitting the data well (minimizing the original Loss term, which risks overfitting).\n",
    "2. Keeping the model simple (minimizing the Penalty term, which risks underfitting).\n",
    "\n",
    "### Effect of $\\lambda$ / $\\alpha$ on Model Behavior\n",
    "\n",
    "| $\\lambda$ / $\\alpha$ Value | Regularization Strength | Effect on Coefficients | Model Outcome |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **$\\lambda = 0$** | **Zero** (No regularization) | Coefficients are unconstrained. | Model reverts to **Ordinary Least Squares (OLS)**. High risk of overfitting. |\n",
    "| **Small $\\lambda$** | **Weak** | Small penalty; coefficients are shrunk slightly. | Model is complex. Still risks overfitting. |\n",
    "| **Optimal $\\lambda$** | **Balanced** | Achieves the best trade-off between bias and variance. | **Optimal Generalization** (the goal). |\n",
    "| **Large $\\lambda$** | **Strong** | Coefficients are heavily penalized (pushed very close to zero). | Model becomes overly simple. High risk of **underfitting** (high bias). |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bce060",
   "metadata": {},
   "source": [
    "### $\\lambda$ / $\\alpha$ Specific Role for L1 and L2\n",
    "\n",
    "| Regularization Type | Penalty Term | The Role of $\\lambda$ |\n",
    "| :--- | :--- | :--- |\n",
    "| **L2 (Ridge)** | $\\lambda \\sum \\theta_i^2$ (Sum of **squared** coefficients) | $\\lambda$ controls the overall **magnitude** of coefficients, making them small without eliminating any features. |\n",
    "| **L1 (Lasso)** | $\\lambda \\sum \\theta_i$ (Sum of **absolute values** of coefficients) | $\\lambda$ controls the **number of features used**. A higher $\\lambda$ forces more unimportant features' coefficients to become **exactly zero** (Feature Selection). |\n",
    "\n",
    "Specific Action on Model Coefficients:\n",
    "1. L2 Regularization (Ridge)\n",
    " - Primary Action: The λ value controls the overall magnitude of the coefficients.\n",
    " - Mechanism: It works by making coefficients small or shrinking them toward zero. Coefficients with very high weights are penalized more aggressively to reduce their individual influence on the prediction.\n",
    " - Result: Coefficients are reduced but never set exactly to zero, meaning no features are eliminated. The model maintains all features but with smaller, more evenly distributed weights.\n",
    "\n",
    "2. L1 Regularization (Lasso)\n",
    " - Primary Action: The λ value controls feature selection and model sparsity.\n",
    " - Mechanism: It penalizes the absolute value of the coefficients. While it shrinks all coefficients, it is mathematically more aggressive, forcing the weights of irrelevant or redundant features to become exactly zero.\n",
    " - Result: It effectively removes the effect of useless features from the prediction, achieving the goal of feature selection.\n",
    "\n",
    " | Regularization Type | Coefficient Treatment | Feature Status |\n",
    "| :--- | :--- | :--- |\n",
    "| **L2 (Ridge)** | Shrinks coefficients (minimizes their effects) | Features are retained but never zero|\n",
    "| **L1 (Lasso)** | Shrinks coefficients and sets irrelevant ones to zero |Features are eliminated / feature selection |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc99234",
   "metadata": {},
   "source": [
    "Elastic Net (Optional but Recommended): A hybrid of Ridge and Lasso."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
