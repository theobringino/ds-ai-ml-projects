{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a88ae37",
   "metadata": {},
   "source": [
    "# Week 5 Notes\n",
    "### Primary Focus: Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9566fa5c",
   "metadata": {},
   "source": [
    "##### What is Feature Engineering?\n",
    "[Reference: Kaggle](https://www.kaggle.com/code/prashant111/a-reference-guide-to-feature-engineering-methods#1.-Introduction-to-Feature-Engineering-)\n",
    "\n",
    "**Feature Engineering** is the process of using domain knowledge to extract features from raw data via data mining techniques to improve the performance of machine earning algorithms.\n",
    "\n",
    "Coming up with features is difficult, time-consuming, and requires expert knowledge. \"Applied machine learning\" is basically feature engineering. - Andrew Ng\n",
    "\n",
    "If we can boil it down to one concept, it's about transforming raw data into a form that's more useful for your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2e7269",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "Models that rely on the **distance between data points** (like k-nearest neighbors or Support Vector Machines) are **highly sensitive to the scale of the features**. If one feature has a much larger range than others, it can dominate the distance calculations and skew the results.\n",
    "\n",
    "There will be other types discussed on other references.\n",
    "\n",
    "- **StandardScaler**\n",
    "    - This technique standardizes features by removing the mean and scaling to unit variance. \n",
    "    - The formula is z=(x−u)/s, where u is the mean and s is the standard deviation. It's great for features that follow a **normal or near-normal distribution**.\n",
    "    $$z=(x−u)/s$$\n",
    "- **MinMaxScaler**\n",
    "    -  This scales features to a fixed range, typically 0 to 1. \n",
    "    - The formula is Xscaled​ = (X−Xmin​)/(Xmax−Xmin). \n",
    "        $$X_{scaled} = \\frac{X - X_{min}}{X_{max} - X_{min}}$$\n",
    "    - This is useful when you need to **constrain your data to a specific range or when your data isn't normally distributed**.\n",
    "\n",
    " Scaling doesn't change the distribution of your data, it just changes the scale. This helps ensure all features contribute equally to the model.\n",
    "\n",
    "### Categorical Encoding \n",
    "Most machine learning algorithms require numerical input. Categorical features (like 'city' or 'product type') need to be converted to numbers before being used.\n",
    "- OneHotEncoder\n",
    "    - This is a common method for handling nominal (non-ordered) categorical data. It creates a new binary column for each category, with a **1 indicating the presence of that category and a 0 for its absence**. For example, if you have a 'city' feature with values 'New York', 'London', and 'Paris', OneHotEncoder would create three new columns: 'city_New York', 'city_London', and 'city_Paris'. This **prevents the model from assuming an arbitrary ordinal relationship between categories**.\n",
    "\n",
    "### Creating new features\n",
    "This is where the \"engineering\" comes in. By combining or transforming existing features, you can capture more complex relationships\n",
    "\n",
    "- **Polynomial features**\n",
    "    - You can create new features by raising existing features to a power e.g., $$x_1^2, x_2^2$$\n",
    "    - This **allows linear models to capture non-linear relationships**.\n",
    "- **Interaction terms**\n",
    "    - These are new features created by multiplying two or more existing features e.g., $$x_1 * x_2$$ \n",
    "    - This allows the model to capture the combined effect of two features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d8616d",
   "metadata": {},
   "source": [
    "Plan of study: \n",
    "- Go over each reference on Day 2 to 3 and implement on Day 4.\n",
    "\n",
    "References:\n",
    "- [Kaggle: A Reference Guide to Feature Engineering Methods](https://www.kaggle.com/code/prashant111/a-reference-guide-to-feature-engineering-methods)\n",
    "- [Geeks For Geeks: What is Feature Engineering?](https://www.geeksforgeeks.org/machine-learning/what-is-feature-engineering/)\n",
    "- [Datacamp: Feature Engineering in Machine Learning: A Practical Guide](https://www.datacamp.com/tutorial/feature-engineering)\n",
    "- [Practical Guide on Data Preprocessing in Python using Scikit Learn](https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/#:~:text=It%20should%20be%20kept%20in,due%20to%20its%20larger%20range.)\n",
    "    - Downloaded the pdf here as it is required to login.\n",
    "- [Data Preprocessing with Scikit-learn](https://medium.com/@drpa/data-preprocessing-with-scikit-learn-dcaaf82d000a)\n",
    "- [10 Powerful Techniques for Feature Engineering in Machine Learning](https://thetechthinker.com/feature-engineering-in-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eec105",
   "metadata": {},
   "source": [
    "Additional items to study/read up on for later weeks:\n",
    "\n",
    "Can do this on the 5th or 6th day along with Cybersecurity this week since the activity is just a research. \n",
    "- Pipelines/orchestration tools (Airflow, Prefect, dbt)\n",
    "- Just a background here, no need for a deep dive. Add vids for tutorials if possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecb64dc",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4416ca",
   "metadata": {},
   "source": [
    "## Kaggle: A Reference Guide to Feature Engineering Methods\n",
    "[Kaggle: A Reference Guide to Feature Engineering Methods](https://www.kaggle.com/code/prashant111/a-reference-guide-to-feature-engineering-methods)\n",
    "\n",
    "Feature engineering is a very broad term that consists of different techniques to process data. These techniques help us to process our raw data into processed data ready to be fed into a machine learning algorithm. These techniques include filling missing values, encode categorical variables, variable transformation, create new variables from existing ones and others.\n",
    "\n",
    "There will be a separate Lab for this one under feature_engineering_kaggle.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab41684",
   "metadata": {},
   "source": [
    "This reference covers six items, namely:\n",
    "1. Missing data imputation\n",
    "2. Categorical Encoding\n",
    "3. Variable Transformation\n",
    "4. Discretization\n",
    "5. Outlier Engineering\n",
    "6. Date and Time Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952edb7c",
   "metadata": {},
   "source": [
    "### Missing Data Imputation\n",
    "\n",
    "- Missing data, or Missing values, occur when no data / no value is stored for a certain observation within a variable.\n",
    "- Missing data are a common occurrence and can have a significant effect on the conclusions that can be drawn from the data. \n",
    "- Incomplete data is an unavoidable problem in dealing with most data sources.\n",
    "- Imputation is the act of replacing missing data with statistical estimates of the missing values. \n",
    "- The goal of any imputation technique is to produce a complete dataset that can be used to train machine learning models.\n",
    "\n",
    "There are multiple techniques for missing data imputation. These are as follows:-\n",
    "1. Complete case analysis\n",
    "2. Mean / Median / Mode imputation\n",
    "3. Random Sample Imputation\n",
    "4. Replacement by Arbitrary Value\n",
    "5. End of Distribution Imputation\n",
    "6. Missing Value Indicator\n",
    "7. Multivariate imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423a3ce8",
   "metadata": {},
   "source": [
    "### Missing Data Mechanisms\n",
    "There are 3 mechanisms that lead to missing data and 2 of them involve missing data randomly or almost randomly with the third one caused by a systematic loss of data.\n",
    "\n",
    "1. Missing Completely at Random, MCAR\n",
    "\n",
    "    A variable is missing completely at random (MCAR) if the probability of being missing is the same for all the observations. When data is MCAR, there is absolutely no relationship between the data missing and any other values, observed or missing, within the dataset. In other words, those missing data points are a random subset of the data. There is nothing systematic going on that makes some data more likely to be missing than other.\n",
    "\n",
    "    If values for observations are missing completely at random, then disregarding those cases would not bias the inferences made.\n",
    "\n",
    "2. Missing at Random, MAR\n",
    "\n",
    "    MAR occurs when there is a systematic relationship between the propensity of missing values and the observed data. In other words, the probability an observation being missing depends only on available information (other variables in the dataset). For example, if men are more likely to disclose their weight than women, weight is MAR. The weight information will be missing at random for those men and women that decided not to disclose their weight, but as men are more prone to disclose it, there will be more missing values for women than for men.\n",
    "\n",
    "    In a situation like the above, if we decide to proceed with the variable with missing values (in this case weight), we might benefit from including gender to control the bias in weight for the missing observations.\n",
    "\n",
    "3. Missing Not at Random, MNAR\n",
    "    \n",
    "    Missing of values is not at random (MNAR) if their being missing depends on information not recorded in the dataset. In other words, there is a mechanism or a reason why missing values are introduced in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e6bd6f",
   "metadata": {},
   "source": [
    "**Noting here**\n",
    "\n",
    "From here on out, the lab will contain the sample codes and the notes will be in this document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d84674",
   "metadata": {},
   "source": [
    "### 1. Complete Case Analysis\n",
    "\n",
    "Complete case analysis implies analysing only those observations in the dataset that contain values in all the variables. In other words, in complete case analysis we remove all observations with missing values. This procedure is suitable when there are few observations with missing data in the dataset.\n",
    "\n",
    "So complete-case analysis (CCA), also called list-wise deletion of cases, consists in simply discarding observations where values in any of the variables are missing. Complete Case Analysis means literally analysing only those observations for which there is information in all of the variables (Xs).\n",
    "\n",
    "But, if the dataset contains missing data across multiple variables, or some variables contain a high proportion of missing observations, we can easily remove a big chunk of the dataset, and this is undesirable.\n",
    "\n",
    "CCA can be applied to both categorical and numerical variables.\n",
    "\n",
    "In practice, CCA may be an acceptable method when the amount of missing information is small. In many real life datasets, the amount of missing data is never small, and therefore CCA is typically never an option.\n",
    "\n",
    "So, in datasets with many variables that contain missing data, CCA will typically not be an option as it will produce a reduced dataset with complete observations. However, if only a subset of the variables from the dataset will be used, we could evaluate variable by variable, whether we choose to discard values with NA, or to replace them with other methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c0b5a9",
   "metadata": {},
   "source": [
    "### 2. Mean/Median/Mode Imputation\n",
    "\n",
    "We can replace missing values with the mean, median or mode of the variable. Mean / median / mode imputation is widely adopted in organisations and data competitions. Although in practice this technique is used in almost every situation, the procedure is suitable if data is missing at random and in small proportions. If there are a lot of missing observations, however, we will distort the distribution of the variable, as well as its relationship with other variables in the dataset. Distortion in the variable distribution may affect the performance of linear models.\n",
    "\n",
    "Mean/median imputation consists of replacing all occurrences of missing values (NA) within a variable by the mean (if the variable has a Gaussian distribution) or median (if the variable has a skewed distribution).\n",
    "\n",
    "For categorical variables, replacement by the mode, is also known as replacement by the most frequent category.\n",
    "\n",
    "Mean/median imputation has the assumption that the data are missing completely at random (MCAR). If this is the case, we can think of replacing the NA with the most frequent occurrence of the variable, which is the mean if the variable has a Gaussian distribution, or the median otherwise.\n",
    "\n",
    "The rationale is to replace the population of missing values with the most frequent value, since this is the most likely occurrence.\n",
    "\n",
    "When replacing NA with the mean or median, the variance of the variable will be distorted if the number of NA is big respect to the total number of observations (since the imputed values do not differ from the mean or from each other). Therefore leading to underestimation of the variance.\n",
    "\n",
    "In addition, estimates of covariance and correlations with other variables in the dataset may also be affected. This is because we may be destroying intrinsic correlations since the mean/median that now replace NA will not preserve the relation with the remaining variables\n",
    "\n",
    "Imputation should be done over the training set, and then propagated to the test set. This means that the mean/median to be used to fill missing values both in train and test set, should be extracted from the train set only. And this is to avoid overfitting.\n",
    "\n",
    "Mean/Median/Mode imputation is the most common method to impute missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c721de8",
   "metadata": {},
   "source": [
    "### 3. Random Sample Imputation\n",
    "\n",
    "Random sample imputation refers to randomly selecting values from the variable to replace the missing data. This technique preserves the variable distribution, and is well suited for data missing at random. But, we need to account for randomness by adequately setting a seed. Otherwise, the same missing observation could be replaced by different values in different code runs, and therefore lead to a different model predictions. This is not desirable when using our models within an organisation.\n",
    "\n",
    "Replacing of NA by random sampling for categorical variables is exactly the same as for numerical variables.\n",
    "\n",
    "Random sampling consist of taking a random observation from the pool of available observations of the variable, that is, from the pool of available categories, and using that randomly extracted value to fill the NA. In Random Sampling one takes as many random observations as missing values are present in the variable.\n",
    "\n",
    "By random sampling observations of the present categories, we guarantee that the frequency of the different categories/labels within the variable is preserved.\n",
    "\n",
    "Assumptions:\n",
    "Random sample imputation has the assumption that the data are missing completely at random (MCAR). If this is the case, it makes sense to substitute the missing values, by values extracted from the original variable distribution/ category frequency.\n",
    "\n",
    "\n",
    "Important Note\n",
    "Imputation should be done over the training set, and then propagated to the test set. This means that the random sample to be used to fill missing values both in train and test set, should be extracted from the train set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5c66d1",
   "metadata": {},
   "source": [
    "### 4. Replacement by Arbitrary Value\n",
    "\n",
    "Replacement by an arbitrary value, as its names indicates, refers to replacing missing data by any, arbitrarily determined value, but the same value for all missing data. Replacement by an arbitrary value is suitable if data is not missing at random, or if there is a huge proportion of missing values. If all values are positive, a typical replacement is -1. Alternatively, replacing by 999 or -999 are common practice. We need to anticipate that these arbitrary values are not a common occurrence in the variable. Replacement by arbitrary values however may not be suited for linear models, as it most likely will distort the distribution of the variables, and therefore model assumptions may not be met.\n",
    "For categorical variables, this is the equivalent of replacing missing observations with the label “Missing” which is a widely adopted procedure.\n",
    "\n",
    "Replacing the NA by artitrary values should be used when there are reasons to believe that the NA are not missing at random. In situations like this, we would not like to replace with the median or the mean, and therefore make the NA look like the majority of our observations.\n",
    "\n",
    "Instead, we want to flag them. We want to capture the missingness somehow.\n",
    "\n",
    "The arbitrary value has to be determined for each variable specifically.\n",
    "We can see that this is totally arbitrary. But, it is used in the industry. Typical values chosen by companies are -9999 or 9999, or similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc13094d",
   "metadata": {},
   "source": [
    "### 5. End of Distribution Imputation\n",
    "End of tail imputation involves replacing missing values by a value at the far end of the tail of the variable distribution. This technique is similar in essence to imputing by an arbitrary value. However, by placing the value at the end of the distribution, we need not look at each variable distribution individually, as the algorithm does it automatically for us. This imputation technique tends to work well with tree-based algorithms, but it may affect the performance of linear models, as it distorts the variable distribution.\n",
    "\n",
    "On occasions, one has reasons to suspect that missing values are not missing at random. And if the value is missing, there has to be a reason for it. Therefore, we would like to capture this information.\n",
    "\n",
    "Adding an additional variable indicating missingness may help with this task. However, the values are still missing in the original variable, and they need to be replaced if we plan to use the variable in machine learning.\n",
    "\n",
    "So, we will replace the NA, by values that are at the far end of the distribution of the variable.\n",
    "\n",
    "The rationale is that if the value is missing, it has to be for a reason, therefore, we would not like to replace missing values for the mean and make that observation look like the majority of our observations. Instead, we want to flag that observation as different, and therefore we assign a value that is at the tail of the distribution, where observations are rarely represented in the population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7600e21",
   "metadata": {},
   "source": [
    "### 6. Missing Value Indicator\n",
    "\n",
    "The missing indicator technique involves adding a binary variable to indicate whether the value is missing for a certain observation. This variable takes the value 1 if the observation is missing, or 0 otherwise. One thing to notice is that we still need to replace the missing values in the original variable, which we tend to do with mean or median imputation. By using these 2 techniques together, if the missing value has predictive power, it will be captured by the missing indicator, and if it doesn’t it will be masked by the mean / median imputation.\n",
    "\n",
    "These 2 techniques in combination tend to work well with linear models. But, adding a missing indicator expands the feature space and, as multiple variables tend to have missing values for the same observations, many of these newly created binary variables could be identical or highly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41294c69",
   "metadata": {},
   "source": [
    "### Conclusion - When to use each imputation method?\n",
    "If missing values are less than 5% of the variable, then go for mean/median imputation or random sample replacement. Impute by most frequent category if missing values are more than 5% of the variable. Do mean/median imputation+adding an additional binary variable to capture missingness add a 'Missing' label in categorical variables.\n",
    "\n",
    "If the number of NA in a variable is small, they are unlikely to have a strong impact on the variable / target that you are trying to predict. Therefore, treating them specially, will most certainly add noise to the variables. Therefore, it is more useful to replace by mean/random sample to preserve the variable distribution.\n",
    "\n",
    "If the variable / target you are trying to predict is however highly unbalanced, then it might be the case that this small number of NA are indeed informative.\n",
    "\n",
    "#### Exceptions\n",
    "If we suspect that NAs are not missing at random and do not want to attribute the most common occurrence to NA, and if we don't want to increase the feature space by adding an additional variable to indicate missingness - in these cases, replace by a value at the far end of the distribution or an arbitrary value."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
